---
title: "Defining Custom Tuner"
author: "Daniel Schalk"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Defining Custom Tuner}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(GenSA)
library(mlr3tuning)
knitr::opts_knit$set(
  datatable.print.keys = FALSE,
  datatable.print.class = TRUE
)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
set.seed(123)
```

`mlr3tuning` is an extension of `mlr3` that includes tuning.

-   Goal: Define custom class with a custom tuner

## Basis of Tuning

As mentioned in the `tuning-01-fitness-function` vignette, we have to define a `[FitnessFunction]` object to define the hyperparameter space as well as the evaluation technique that is used to estimate the generalization error:

```{r}
task = mlr3::mlr_tasks$get("spam")
learner = mlr3::mlr_learners$get("classif.rpart")
learner$predict_type = "prob"
resampling = mlr3::mlr_resamplings$get("holdout")
measures = mlr3::mlr_measures$mget(c("classif.auc", "classif.ce"))
task$measures = measures
param_set = paradox::ParamSet$new(
  params = list(
    paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1)
    # paradox::ParamInt$new("minsplit", lower = 2, upper = 5)
  )
)

ff = FitnessFunction$new(task, learner, resampling, param_set)
```

## Define Objective


- Return negative performance measure in case of optimization (like the AUC)

```{r}
blackBoxFun = function (x, ff) {
  x = mlr3misc::set_names(x, nm = ff$param_set$ids())
  ff$eval(x)
  performance = unlist(ff$bmr$data[.N]$performance)[[1]]
  if (! ff$task$measures[[1]]$minimize)
    return (-performance)
  return (performance)
}
```

This function ..

-   .. runs the fitness function `ff` for the parameter value `x`.
-   .. evaluates the learner using the given `resampling` object and adds the resampling result to the benchmark result object of the fitness function .

```{r}
# blackBoxFun(c(cp = 0.05, minsplit = 5), ff)
blackBoxFun(c(cp = 0.05), ff)
ff$bmr$aggregated()

# blackBoxFun(c(cp = 0.2, minsplit = 3), ff)
blackBoxFun(c(cp = 0.2), ff)
ff$bmr$aggregated()
```


## Call the Optimizer

```{r, eval=FALSE}
library(GenSA)
res = GenSA(fn = blackBoxFun, lower = param_set$lower, upper = param_set$upper, ff = ff)
```


## Terminator of Custom Tuner

To define a custom tuner class we also have to think about how the terminator are expressed by `GenSA`. To keep it simple we are just using the evaluations terminator to control the tuning behavior.

The number of evaluations of the fitness function can be controlled by `max.call`. However, by setting `max.call` we don't get the real number of evaluations since `GenSA` finishes one iteration first and then stops. Therefore, it might happen that we specify 120 evaluation as maximum, but the algorithm calls the fitness function 126 times.

```{r}
res = GenSA(fn = blackBoxFun, lower = param_set$lower, upper = param_set$upper,
  control = list(max.call = 60L), ff = ff)
```

```{r}
res$counts
```


## Write New Class

Instead of running `GenSA` with our custom objective, we now want to write a class to use the whole `mlr3` infrastructure for the custom tuner.

Basically, each tuner consists of the constructor (the `$new()` initialization) and a `$tune_step()` method. The major part of custom tuning is already done by defining the objective that uses the fitness function. The last step now is to bring the objective together with the terminator. Since `mlr3` is written with `R6` we load the package:

```{r}
library(R6)
```

For our custom tuner the class skeleton looks like this:

```{r, eval=FALSE}
TunerGridSearch = R6Class("TunerGenSA",
  inherit = Tuner,
  public = list(
    initialize = function(ff, terminator) { ... }
  ),
  private = list(
    tune_step = function() { ... }
  )
)

```

Finally, we have to define the two functions:

`$initialize()`:
-   Checks: for GenSA everything needs to be a double or integer

`$tune_step()`:
-   bla

To keep it simple we are just using the evaluation terminator here


```{r}
TunerGenSA = R6Class("TunerGenSA",
  inherit = Tuner,
  public = list(
    GenSA_res = NULL,
    initialize = function(ff, evals, ...) {
      if (any(param_set$storage_type != "numeric")) {
        stop("Parameter types needs to be numeric")
      }
      checkmate::assert_integerish(evals, lower = 1L)
      super$initialize(id = "GenSA", ff = ff, terminator = TerminatorEvaluations$new(evals),
        settings = list(max.call = evals, ...))
    }
  ),
  private = list(
    tune_step = function() {
      blackBoxFun = function (x, ff) {
        x = mlr3misc::set_names(x, nm = ff$param_set$ids())
        ff$eval(x)
        performance = unlist(ff$bmr$data[.N]$performance)[[1]]
        if (! ff$task$measures[[1]]$minimize)
          return (-performance)
        return (performance)
      }
      self$GenSA_res = GenSA(fn = blackBoxFun, lower = self$ff$param_set$lower, upper = self$ff$param_set$upper,
        control = self$settings, ff = self$ff)
    }
  )
)
```

## Run Custom Tuner

Complete example:

```{r}
task = mlr3::mlr_tasks$get("spam")
learner = mlr3::mlr_learners$get("classif.rpart")
learner$predict_type = "prob"
resampling = mlr3::mlr_resamplings$get("holdout")
measures = mlr3::mlr_measures$mget(c("classif.auc", "classif.ce"))
param_set = paradox::ParamSet$new(
  params = list(
    paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1)
    # paradox::ParamInt$new("minsplit", lower = 2, upper = 5)
  )
)

ff = FitnessFunction$new(task, learner, resampling, param_set)
tuner = TunerGenSA$new(ff, 60L)
tuner$tune()

tuner$ff$bmr$aggregated()
tuner$tune_result()
str(tuner$GenSA_res)
```
