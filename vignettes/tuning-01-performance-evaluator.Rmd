---
title: "Introduction to Performance Evaluators"
author: "Daniel Schalk"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to Performance Evaluators}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(mlr3tuning)
knitr::opts_knit$set(
  datatable.print.keys = FALSE,
  datatable.print.class = TRUE
)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
set.seed(123)
```

`mlr3tuning` is an extension of `mlr3` that includes tuning.

## Basis of Tuning

Before we are able to tune hyperparameters, it is necessary to define the learner, task, how to evaluate a hyperparameter setting, and the hyperparameter space. Here, we will use the [iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) and a decision tree from `rpart`:

```{r}
task = mlr3::mlr_tasks$get("iris")
learner = mlr3::mlr_learners$get("classif.rpart")
resampling = mlr3::mlr_resamplings$get("cv")
measures = mlr3::mlr_measures$mget("classif.ce")
task$measures = measures
param_set = paradox::ParamSet$new(params = list(paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1)))
```

## Define Performance Evaluator

With that basis we can define a performance evaluator:

```{r}
pe = PerformanceEvaluator$new(
  task = task,
  learner = learner,
  resampling = resampling,
  param_set = param_set
)
```

At this point there are no results since have not computed anything jet:
```{r}
pe$bmr
```

However, the defined parameter space that is used for later search strategies is stored within the public member `param_set`:
```{r}
pe$param_set
```

## Evaluate Parameter Configurations

With the `$eval()` member function we can evaluate a single parameter configuration given as named list or vector:
```{r}
pe$eval(c(cp = 0.05, minsplit = 5))
```

Since we have defined 10 fold cross validation as resampling stragety the model is trained and evaluated 10 times. All evaluation results are stored as `mlr3::BenchmarkResult`:
```{r}
pe$bmr
pe$bmr$data
```

With `pe$bmr$aggregated` we get a summary per resampling:
```{r}
pe$bmr$aggregated()
```

If we use another parameter configuration we, again, observe that the results are automatically inserted into the benchmark result:
```{r}
pe$eval(c(cp = 0.05, minsplit = 2))
pe$bmr$aggregated()
```

Note that `$eval()` is the basis of the tuning. The member function evaluates a black box function defined by the model and evaluates the parameter setting using the given resampling strategy combined by the performance evaluator.
