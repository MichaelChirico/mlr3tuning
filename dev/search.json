[{"path":"https://mlr3tuning.mlr-org.com/dev/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Marc Becker. Maintainer, author. Michel Lang. Author. Jakob Richter. Author. Bernd Bischl. Author. Daniel Schalk. Author.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Becker M, Lang M, Richter J, Bischl B, Schalk D (2021). mlr3tuning: Tuning 'mlr3'. https://mlr3tuning.mlr-org.com, https://github.com/mlr-org/mlr3tuning.","code":"@Manual{,   title = {mlr3tuning: Tuning for 'mlr3'},   author = {Marc Becker and Michel Lang and Jakob Richter and Bernd Bischl and Daniel Schalk},   year = {2021},   note = {https://mlr3tuning.mlr-org.com, https://github.com/mlr-org/mlr3tuning}, }"},{"path":"https://mlr3tuning.mlr-org.com/dev/index.html","id":"mlr3tuning","dir":"","previous_headings":"","what":"Tuning for mlr3","title":"Tuning for mlr3","text":"Package website: release | dev package provides hyperparameter tuning mlr3. offers various tuning methods e.g.Â grid search, random search generalized simulated annealing iterated racing different termination criteria can set combined. AutoTuner provides convenient way perform nested resampling combination mlr3. package build bbotk provides common framework optimization.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/index.html","id":"extension-packages","dir":"","previous_headings":"","what":"Extension packages","title":"Tuning for mlr3","text":"mlr3tuningspaces offers collection search spaces hyperparameter tuning. mlr3hyperband adds hyperband successive halving algorithm.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/index.html","id":"resources","dir":"","previous_headings":"","what":"Resources","title":"Tuning for mlr3","text":"mlr3book chapters tuning search spaces, hyperparameter tuning nested resampling. mlr3gallery posts cheatsheet","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tuning for mlr3","text":"Install last release CRAN: Install development version GitHub:","code":"install.packages(\"mlr3tuning\") remotes::install_github(\"mlr-org/mlr3tuning\")"},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/index.html","id":"basic-hyperparameter-tuning","dir":"","previous_headings":"Example","what":"Basic hyperparameter tuning","title":"Tuning for mlr3","text":"","code":"library(\"mlr3tuning\")  # retrieve task task = tsk(\"pima\")  # load learner and set search space learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE))  # hyperparameter tuning on the pima indians diabetes data set instance = tune(   method = \"random_search\",   task = task,   learner = learner,   resampling = rsmp(\"cv\", folds = 3),   measure = msr(\"classif.ce\"),   term_evals = 10,   batch_size = 5 )  # best performing hyperparameter configuration instance$result ##           cp learner_param_vals  x_domain classif.ce ## 1: -2.774656          <list[2]> <list[1]>  0.2617188 # all evaluated hyperparameter configuration as.data.table(instance$archive) ##            cp classif.ce  x_domain_cp runtime_learners           timestamp batch_nr      resample_result ##  1: -6.355096  0.2786458 0.0017378683            0.032 2021-10-27 14:24:43        1 <ResampleResult[22]> ##  2: -5.937751  0.2799479 0.0026379549            0.035 2021-10-27 14:24:43        1 <ResampleResult[22]> ##  3: -4.280177  0.2734375 0.0138402055            0.060 2021-10-27 14:24:43        1 <ResampleResult[22]> ##  4: -7.539553  0.2786458 0.0005316351            0.107 2021-10-27 14:24:43        1 <ResampleResult[22]> ##  5: -7.140496  0.2786458 0.0007923588            0.037 2021-10-27 14:24:43        1 <ResampleResult[22]> ##  6: -9.114735  0.2786458 0.0001100325            0.030 2021-10-27 14:24:44        2 <ResampleResult[22]> ##  7: -5.401998  0.2695312 0.0045075636            0.034 2021-10-27 14:24:44        2 <ResampleResult[22]> ##  8: -4.703298  0.2773438 0.0090653290            0.032 2021-10-27 14:24:44        2 <ResampleResult[22]> ##  9: -2.774656  0.2617188 0.0623709163            0.031 2021-10-27 14:24:44        2 <ResampleResult[22]> ## 10: -4.441283  0.2734375 0.0117808090            0.030 2021-10-27 14:24:44        2 <ResampleResult[22]> # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(task)"},{"path":"https://mlr3tuning.mlr-org.com/dev/index.html","id":"automatic-tuning","dir":"","previous_headings":"Example","what":"Automatic tuning","title":"Tuning for mlr3","text":"","code":"# retrieve task task = tsk(\"pima\")  # construct auto tuner at = auto_tuner(   method = \"random_search\",   learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),   resampling = rsmp(\"cv\", folds = 3),   measure = msr(\"classif.ce\"),   term_evals = 10,   batch_size = 5 )  # train/test split train_set = sample(task$nrow, 0.8 * task$nrow) test_set = setdiff(seq_len(task$nrow), train_set)  # tune hyperparameters and fit final model on the complete data set in one go at$train(task, row_ids = train_set)  # best performing hyperparameter configuration at$tuning_result ##           cp learner_param_vals  x_domain classif.ce ## 1: -4.159136          <list[2]> <list[1]>  0.2590228 # all evaluated hyperparameter configuration as.data.table(at$archive) ##            cp classif.ce  x_domain_cp runtime_learners           timestamp batch_nr      resample_result ##  1: -6.469604  0.2671051 0.0015498397            0.037 2021-10-27 14:24:44        1 <ResampleResult[22]> ##  2: -5.975552  0.2671051 0.0025400997            0.034 2021-10-27 14:24:44        1 <ResampleResult[22]> ##  3: -6.304555  0.2671051 0.0018279594            0.032 2021-10-27 14:24:44        1 <ResampleResult[22]> ##  4: -2.885359  0.2703730 0.0558347447            0.033 2021-10-27 14:24:44        1 <ResampleResult[22]> ##  5: -5.878631  0.2671051 0.0027986148            0.031 2021-10-27 14:24:44        1 <ResampleResult[22]> ##  6: -5.316384  0.2671051 0.0049104786            0.030 2021-10-27 14:24:45        2 <ResampleResult[22]> ##  7: -4.159136  0.2590228 0.0156210471            0.030 2021-10-27 14:24:45        2 <ResampleResult[22]> ##  8: -9.206399  0.2671051 0.0001003949            0.033 2021-10-27 14:24:45        2 <ResampleResult[22]> ##  9: -4.869814  0.2785350 0.0076747945            0.032 2021-10-27 14:24:45        2 <ResampleResult[22]> ## 10: -6.649454  0.2671051 0.0012947286            0.030 2021-10-27 14:24:45        2 <ResampleResult[22]> # predict new data with model trained on the complete data set and optimized hyperparameters at$predict(task, row_ids = test_set) ## <PredictionClassif> for 154 observations: ##     row_ids truth response ##           3   pos      pos ##           6   neg      neg ##          11   neg      neg ## ---                        ##         756   pos      pos ##         758   pos      pos ##         768   neg      neg"},{"path":"https://mlr3tuning.mlr-org.com/dev/index.html","id":"nested-resampling","dir":"","previous_headings":"Example","what":"Nested resampling","title":"Tuning for mlr3","text":"","code":"# retrieve task task = tsk(\"pima\")  # load learner and set search space learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE))  # nested resampling rr = tune_nested(   method = \"random_search\",   task =  task,   learner = learner,   inner_resampling = rsmp(\"holdout\"),   outer_resampling = rsmp(\"cv\", folds = 3),   measure = msr(\"classif.ce\"),   term_evals = 10,   batch_size = 5 )  # aggregated performance of all outer resampling iterations rr$aggregate() ## classif.ce  ##  0.2473958 # performance scores of the outer resampling rr$score() ##                 task task_id         learner          learner_id         resampling resampling_id iteration              prediction classif.ce ## 1: <TaskClassif[49]>    pima <AutoTuner[41]> classif.rpart.tuned <ResamplingCV[19]>            cv         1 <PredictionClassif[20]>  0.2187500 ## 2: <TaskClassif[49]>    pima <AutoTuner[41]> classif.rpart.tuned <ResamplingCV[19]>            cv         2 <PredictionClassif[20]>  0.2500000 ## 3: <TaskClassif[49]>    pima <AutoTuner[41]> classif.rpart.tuned <ResamplingCV[19]>            cv         3 <PredictionClassif[20]>  0.2734375 # inner resampling results extract_inner_tuning_results(rr) ##    iteration        cp classif.ce learner_param_vals  x_domain task_id          learner_id resampling_id ## 1:         1 -2.768620  0.2573099          <list[2]> <list[1]>    pima classif.rpart.tuned            cv ## 2:         2 -3.880799  0.2046784          <list[2]> <list[1]>    pima classif.rpart.tuned            cv ## 3:         3 -8.862942  0.2748538          <list[2]> <list[1]>    pima classif.rpart.tuned            cv # inner resampling archives extract_inner_tuning_archives(rr) ##     iteration        cp classif.ce  x_domain_cp runtime_learners           timestamp batch_nr      resample_result task_id          learner_id resampling_id ##  1:         1 -4.539772  0.2748538 0.0106758449            0.011 2021-10-27 14:24:45        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ##  2:         1 -7.559936  0.2631579 0.0005209086            0.009 2021-10-27 14:24:45        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ##  3:         1 -8.648543  0.2631579 0.0001753822            0.010 2021-10-27 14:24:45        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ##  4:         1 -6.297959  0.2631579 0.0018400560            0.010 2021-10-27 14:24:45        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ##  5:         1 -8.947182  0.2631579 0.0001301033            0.010 2021-10-27 14:24:45        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ##  6:         1 -8.067483  0.2631579 0.0003135715            0.011 2021-10-27 14:24:45        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ##  7:         1 -8.350241  0.2631579 0.0002363396            0.010 2021-10-27 14:24:45        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ##  8:         1 -5.913481  0.2631579 0.0027027622            0.015 2021-10-27 14:24:45        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ##  9:         1 -8.752513  0.2631579 0.0001580636            0.012 2021-10-27 14:24:45        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 10:         1 -2.768620  0.2573099 0.0627485302            0.009 2021-10-27 14:24:45        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 11:         2 -3.286175  0.2105263 0.0373966171            0.010 2021-10-27 14:24:46        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 12:         2 -4.124071  0.2456140 0.0161785202            0.009 2021-10-27 14:24:46        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 13:         2 -2.385855  0.2105263 0.0920102553            0.010 2021-10-27 14:24:46        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 14:         2 -3.880799  0.2046784 0.0206343371            0.016 2021-10-27 14:24:46        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 15:         2 -4.328644  0.2456140 0.0131854101            0.009 2021-10-27 14:24:46        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 16:         2 -4.394274  0.2456140 0.0123478454            0.010 2021-10-27 14:24:46        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 17:         2 -5.922306  0.2690058 0.0026790151            0.009 2021-10-27 14:24:46        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 18:         2 -7.331236  0.2690058 0.0006547636            0.010 2021-10-27 14:24:46        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 19:         2 -4.992721  0.2690058 0.0067871745            0.009 2021-10-27 14:24:46        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 20:         2 -3.362562  0.2105263 0.0346463879            0.009 2021-10-27 14:24:46        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 21:         3 -8.862942  0.2748538 0.0001415380            0.011 2021-10-27 14:24:46        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 22:         3 -7.347079  0.2748538 0.0006444723            0.010 2021-10-27 14:24:46        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 23:         3 -7.067612  0.2748538 0.0008522663            0.014 2021-10-27 14:24:46        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 24:         3 -8.044845  0.2748538 0.0003207512            0.010 2021-10-27 14:24:46        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 25:         3 -6.697964  0.2748538 0.0012334207            0.009 2021-10-27 14:24:46        1 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 26:         3 -6.530743  0.2748538 0.0014579225            0.010 2021-10-27 14:24:47        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 27:         3 -4.267553  0.2982456 0.0140160370            0.010 2021-10-27 14:24:47        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 28:         3 -6.352161  0.2748538 0.0017429764            0.010 2021-10-27 14:24:47        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 29:         3 -6.222573  0.2748538 0.0019841342            0.010 2021-10-27 14:24:47        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ## 30:         3 -3.570023  0.2865497 0.0281551963            0.009 2021-10-27 14:24:47        2 <ResampleResult[22]>    pima classif.rpart.tuned            cv ##     iteration        cp classif.ce  x_domain_cp runtime_learners           timestamp batch_nr      resample_result task_id          learner_id resampling_id"},{"path":"https://mlr3tuning.mlr-org.com/dev/index.html","id":"hotstart","dir":"","previous_headings":"Example","what":"Hotstart","title":"Tuning for mlr3","text":"","code":"library(\"mlr3tuning\") library(\"mlr3learners\")  # retrieve task task = tsk(\"pima\")  # load learner and set search space learner = lrn(\"classif.xgboost\",   eta = to_tune(),   nrounds = to_tune(500, 2500),   eval_metric = \"logloss\" )  # hyperparameter tuning on the pima indians diabetes data set instance = tune(   method = \"grid_search\",   task = task,   learner = learner,   resampling = rsmp(\"cv\", folds = 3),   measure = msr(\"classif.ce\"),   allow_hotstart = TRUE,   resolution = 5,   batch_size = 5 )  # best performing hyperparameter configuration instance$result ##    nrounds eta learner_param_vals  x_domain classif.ce ## 1:    1500 0.5          <list[5]> <list[2]>  0.1302083 # all evaluated hyperparameter configuration as.data.table(instance$archive) ##     nrounds  eta classif.ce x_domain_nrounds x_domain_eta runtime_learners           timestamp batch_nr      resample_result ##  1:     500 0.00  0.4843750              500         0.00            1.426 2021-10-27 14:24:54        1 <ResampleResult[22]> ##  2:     500 0.25  0.2682292              500         0.25            0.890 2021-10-27 14:24:54        1 <ResampleResult[22]> ##  3:     500 0.50  0.2695312              500         0.50            0.750 2021-10-27 14:24:54        1 <ResampleResult[22]> ##  4:     500 0.75  0.2734375              500         0.75            0.715 2021-10-27 14:24:54        1 <ResampleResult[22]> ##  5:     500 1.00  0.2799479              500         1.00            0.697 2021-10-27 14:24:54        1 <ResampleResult[22]> ##  6:    1000 0.00  0.5091146             1000         0.00            1.829 2021-10-27 14:25:00        2 <ResampleResult[22]> ##  7:    1000 0.25  0.1315104             1000         0.25            0.813 2021-10-27 14:25:00        2 <ResampleResult[22]> ##  8:    1000 0.50  0.1471354             1000         0.50            0.702 2021-10-27 14:25:00        2 <ResampleResult[22]> ##  9:    1000 0.75  0.1510417             1000         0.75            0.637 2021-10-27 14:25:00        2 <ResampleResult[22]> ## 10:    1000 1.00  0.1588542             1000         1.00            0.620 2021-10-27 14:25:00        2 <ResampleResult[22]> ## 11:    1500 0.00  0.5130208             1500         0.00            1.999 2021-10-27 14:25:07        3 <ResampleResult[22]> ## 12:    1500 0.25  0.1393229             1500         0.25            0.897 2021-10-27 14:25:07        3 <ResampleResult[22]> ## 13:    1500 0.50  0.1302083             1500         0.50            0.764 2021-10-27 14:25:07        3 <ResampleResult[22]> ## 14:    1500 0.75  0.1497396             1500         0.75            0.701 2021-10-27 14:25:07        3 <ResampleResult[22]> ## 15:    1500 1.00  0.1679688             1500         1.00            0.646 2021-10-27 14:25:07        3 <ResampleResult[22]> ## 16:    2000 0.00  0.4908854             2000         0.00            2.260 2021-10-27 14:25:14        4 <ResampleResult[22]> ## 17:    2000 0.25  0.1302083             2000         0.25            0.981 2021-10-27 14:25:14        4 <ResampleResult[22]> ## 18:    2000 0.50  0.1380208             2000         0.50            0.859 2021-10-27 14:25:14        4 <ResampleResult[22]> ## 19:    2000 0.75  0.1549479             2000         0.75            0.771 2021-10-27 14:25:14        4 <ResampleResult[22]> ## 20:    2000 1.00  0.1679688             2000         1.00            0.727 2021-10-27 14:25:14        4 <ResampleResult[22]> ## 21:    2500 0.00  0.4947917             2500         0.00            2.648 2021-10-27 14:25:22        5 <ResampleResult[22]> ## 22:    2500 0.25  0.1341146             2500         0.25            1.052 2021-10-27 14:25:22        5 <ResampleResult[22]> ## 23:    2500 0.50  0.1393229             2500         0.50            0.910 2021-10-27 14:25:22        5 <ResampleResult[22]> ## 24:    2500 0.75  0.1536458             2500         0.75            0.870 2021-10-27 14:25:22        5 <ResampleResult[22]> ## 25:    2500 1.00  0.1549479             2500         1.00            0.805 2021-10-27 14:25:22        5 <ResampleResult[22]> ##     nrounds  eta classif.ce x_domain_nrounds x_domain_eta runtime_learners           timestamp batch_nr      resample_result # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(task)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":null,"dir":"Reference","previous_headings":"","what":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"Container around data.table::data.table() stores evaluated hyperparameter configurations performance scores.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"data-structure","dir":"Reference","previous_headings":"","what":"Data structure","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"table ($data) following columns: One column hyperparameter search space ($search_space). One column performance measure ($codomain). x_domain (list()) Lists (transformed) hyperparameter values passed learner. runtime_learners (numeric(1)) Sum training predict times logged learners per mlr3::ResampleResult / evaluation. include potential overhead time. timestamp (POSIXct) Time stamp evaluation logged archive. batch_nr (integer(1)) Hyperparameters evaluated batches. batch unique batch number. uhash (character(1)) Connects hyperparameter configuration resampling experiment stored mlr3::BenchmarkResult. row corresponds single evaluation hyperparameter configuration. archive stores additionally mlr3::BenchmarkResult ($benchmark_result) records resampling experiments. experiment corresponds single evaluation hyperparameter configuration. table ($data) benchmark result ($benchmark_result) linked uhash column. results viewed .data.table(), joined automatically.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"analysis","dir":"Reference","previous_headings":"","what":"Analysis","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"analyzing tuning results, recommended pass archive .data.table(). returned data table joined benchmark result adds mlr3::ResampleResult hyperparameter evaluation. archive provides various getters (e.g. $learners()) ease access. getters extract position () unique hash (uhash). complete list getters see methods section. benchmark result ($benchmark_result) allows score hyperparameter configurations different measure. Alternatively, measures can supplied .data.table(). mlr3viz package provides visualizations tuning results.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"s-methods","dir":"Reference","previous_headings":"","what":"S3 Methods","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":".data.table.ArchiveTuning(x, unnest = \"x_domain\", exclude_columns = \"uhash\", measures = NULL) Returns tabular view evaluated hyperparameter configurations. ArchiveTuning -> data.table::data.table() x (ArchiveTuning) unnest (character()) Transforms list columns separate columns. Set NULL column unnested. exclude_columns (character()) Exclude columns table. Set NULL column excluded. measures (List mlr3::Measure) Score hyperparameter configurations additional measures.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"bbotk::Archive -> ArchiveTuning","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"benchmark_result (mlr3::BenchmarkResult).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"bbotk::Archive$add_evals() bbotk::Archive$best() bbotk::Archive$clear() bbotk::Archive$format() bbotk::Archive$nds_selection()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"ArchiveTuning$new() ArchiveTuning$learner() ArchiveTuning$learners() ArchiveTuning$learner_param_vals() ArchiveTuning$predictions() ArchiveTuning$resample_result() ArchiveTuning$print() ArchiveTuning$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"","code":"ArchiveTuning$new(search_space, codomain, check_values = TRUE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). codomain (bbotk::Codomain) Specifies codomain objective function .e. set performance measures. Internally created provided mlr3::Measures. check_values (logical(1)) TRUE (default), hyperparameter configurations check validity.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-learner-","dir":"Reference","previous_headings":"","what":"Method learner()","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"Retrieve mlr3::Learner -th evaluation, position unique hash uhash. uhash mutually exclusive. Learner contain model. Use $learners() get learners models.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"","code":"ArchiveTuning$learner(i = NULL, uhash = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"(integer(1)) iteration value filter . uhash (logical(1)) uhash value filter .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-learners-","dir":"Reference","previous_headings":"","what":"Method learners()","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"Retrieve list trained mlr3::Learner objects -th evaluation, position unique hash uhash. uhash mutually exclusive.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"","code":"ArchiveTuning$learners(i = NULL, uhash = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"(integer(1)) iteration value filter . uhash (logical(1)) uhash value filter .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-learner-param-vals-","dir":"Reference","previous_headings":"","what":"Method learner_param_vals()","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"Retrieve param values -th evaluation, position unique hash uhash. uhash mutually exclusive.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"","code":"ArchiveTuning$learner_param_vals(i = NULL, uhash = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"(integer(1)) iteration value filter . uhash (logical(1)) uhash value filter .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-predictions-","dir":"Reference","previous_headings":"","what":"Method predictions()","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"Retrieve list mlr3::Prediction objects -th evaluation, position unique hash uhash. uhash mutually exclusive.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"","code":"ArchiveTuning$predictions(i = NULL, uhash = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"(integer(1)) iteration value filter . uhash (logical(1)) uhash value filter .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-resample-result-","dir":"Reference","previous_headings":"","what":"Method resample_result()","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"Retrieve mlr3::ResampleResult -th evaluation, position unique hash uhash. uhash mutually exclusive.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"","code":"ArchiveTuning$resample_result(i = NULL, uhash = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"(integer(1)) iteration value filter . uhash (logical(1)) uhash value filter .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"Printer.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"","code":"ArchiveTuning$print()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"... (ignored).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"","code":"ArchiveTuning$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ArchiveTuning.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Logging Object for Evaluated Hyperparameter Configurations â ArchiveTuning","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":null,"dir":"Reference","previous_headings":"","what":"AutoTuner â AutoTuner","title":"AutoTuner â AutoTuner","text":"AutoTuner mlr3::Learner wraps another mlr3::Learner performs following steps $train(): hyperparameters wrapped (inner) learner trained training data via resampling. tuning can specified providing Tuner, bbotk::Terminator, search space paradox::ParamSet, mlr3::Resampling mlr3::Measure. best found hyperparameter configuration set hyperparameters wrapped (inner) learner stored $learner. Access tuned hyperparameters via $learner$param_set$values. final model fit complete training data using now parametrized wrapped learner. respective model available via field $learner$model. $predict() AutoTuner just calls predict method wrapped (inner) learner. set timeout disabled fitting final model. Note approach allows perform nested resampling passing AutoTuner object mlr3::resample() mlr3::benchmark(). access inner resampling results, set store_tuning_instance = TRUE execute mlr3::resample() mlr3::benchmark() store_models = TRUE (see examples).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"AutoTuner â AutoTuner","text":"mlr3::Learner -> AutoTuner","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"AutoTuner â AutoTuner","text":"instance_args (list()) arguments construction create TuningInstanceSingleCrit. tuner (Tuner).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"AutoTuner â AutoTuner","text":"archive ArchiveTuning Archive TuningInstanceSingleCrit. learner (mlr3::Learner) Trained learner tuning_instance (TuningInstanceSingleCrit) Internally created tuning instance intermediate results. tuning_result (data.table::data.table) Short-cut result TuningInstanceSingleCrit. predict_type (character(1)) Stores currently active predict type, e.g. \"response\". Must element $predict_types. hash (character(1)) Hash (unique identifier) object.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"AutoTuner â AutoTuner","text":"mlr3::Learner$format() mlr3::Learner$help() mlr3::Learner$predict() mlr3::Learner$predict_newdata() mlr3::Learner$reset() mlr3::Learner$train()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"AutoTuner â AutoTuner","text":"AutoTuner$new() AutoTuner$base_learner() AutoTuner$print() AutoTuner$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"AutoTuner â AutoTuner","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AutoTuner â AutoTuner","text":"","code":"AutoTuner$new(   learner,   resampling,   measure,   terminator,   tuner,   search_space = NULL,   store_tuning_instance = TRUE,   store_benchmark_result = TRUE,   store_models = FALSE,   check_values = FALSE )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AutoTuner â AutoTuner","text":"learner (mlr3::Learner) Learner tune, see TuningInstanceSingleCrit. resampling (mlr3::Resampling) Resampling strategy tuning, see TuningInstanceSingleCrit. mlr3::Resampling meant inner resampling, operating training set arbitrary outer resampling. reason feasible pass instantiated mlr3::Resampling . measure (mlr3::Measure) Performance measure optimize. terminator (bbotk::Terminator) stop tuning, see TuningInstanceSingleCrit. tuner (Tuner) Tuning algorithm run. search_space (paradox::ParamSet) Hyperparameter search space. NULL, search space constructed TuneToken ParamSet learner. store_tuning_instance (logical(1)) TRUE (default), stores internally created TuningInstanceSingleCrit intermediate results slot $tuning_instance. store_benchmark_result (logical(1)) TRUE (default), store resample result evaluated hyperparameter configurations archive mlr3::BenchmarkResult. store_models (logical(1)) TRUE, fitted models stored benchmark result (archive$benchmark_result). store_benchmark_result = FALSE, models stored temporarily accessible tuning. combination needed measures require model. check_values (logical(1)) TRUE, hyperparameter values checked evaluation performance scores . FALSE (default), values unchecked computational overhead reduced.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"method-base-learner-","dir":"Reference","previous_headings":"","what":"Method base_learner()","title":"AutoTuner â AutoTuner","text":"Extracts base learner nested learner objects like GraphLearner mlr3pipelines. recursive = 0, (tuned) learner returned.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"AutoTuner â AutoTuner","text":"","code":"AutoTuner$base_learner(recursive = Inf)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"AutoTuner â AutoTuner","text":"recursive (integer(1)) Depth recursion multiple nested objects.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"AutoTuner â AutoTuner","text":"Learner. Printer.","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"AutoTuner â AutoTuner","text":"","code":"AutoTuner$print()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"AutoTuner â AutoTuner","text":"... (ignored).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"AutoTuner â AutoTuner","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"AutoTuner â AutoTuner","text":"","code":"AutoTuner$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"AutoTuner â AutoTuner","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/AutoTuner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"AutoTuner â AutoTuner","text":"","code":"task = tsk(\"pima\") train_set = sample(task$nrow, 0.8 * task$nrow) test_set = setdiff(seq_len(task$nrow), train_set)  at = AutoTuner$new(   learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   terminator = trm(\"evals\", n_evals = 5),   tuner = tnr(\"random_search\"))  # tune hyperparameters and fit final model at$train(task, row_ids = train_set)  # predict with final model at$predict(task, row_ids = test_set) #> <PredictionClassif> for 154 observations: #>     row_ids truth response #>           2   neg      neg #>           3   pos      pos #>           8   neg      neg #> ---                        #>         758   pos      neg #>         759   neg      neg #>         765   neg      neg  # show tuning result at$tuning_result #>           cp learner_param_vals  x_domain classif.ce #> 1: -8.058762          <list[2]> <list[1]>  0.2780488  # model slot contains trained learner and tuning instance at$model #> $learner #> <LearnerClassifRpart:classif.rpart> #> * Model: rpart #> * Parameters: xval=0, cp=0.0003163 #> * Packages: mlr3, rpart #> * Predict Type: response #> * Feature types: logical, integer, numeric, factor, ordered #> * Properties: importance, missings, multiclass, selected_features, #>   twoclass, weights #>  #> $tuning_instance #> <TuningInstanceSingleCrit> #> * State:  Optimized #> * Objective: <ObjectiveTuning:classif.rpart_on_pima> #> * Search Space: #> <ParamSet> #>    id    class    lower     upper nlevels        default value #> 1: cp ParamDbl -9.21034 -2.302585     Inf <NoDefault[3]>       #> Trafo is set. #> * Terminator: <TerminatorEvals> #> * Terminated: TRUE #> * Result: #>           cp learner_param_vals  x_domain classif.ce #> 1: -8.058762          <list[2]> <list[1]>  0.2780488 #> * Archive: #> <ArchiveTuning> #>      cp classif.ce runtime_learners           timestamp batch_nr #> 1: -8.1       0.28            0.029 2021-12-26 04:26:21        1 #> 2: -4.2       0.28            0.076 2021-12-26 04:26:21        2 #> 3: -6.9       0.28            0.016 2021-12-26 04:26:21        3 #> 4: -7.9       0.28            0.015 2021-12-26 04:26:21        4 #> 5: -6.9       0.28            0.017 2021-12-26 04:26:21        5 #>         resample_result #> 1: <ResampleResult[22]> #> 2: <ResampleResult[22]> #> 3: <ResampleResult[22]> #> 4: <ResampleResult[22]> #> 5: <ResampleResult[22]> #>   # shortcut trained learner at$learner #> <LearnerClassifRpart:classif.rpart> #> * Model: rpart #> * Parameters: xval=0, cp=0.0003163 #> * Packages: mlr3, rpart #> * Predict Type: response #> * Feature types: logical, integer, numeric, factor, ordered #> * Properties: importance, missings, multiclass, selected_features, #>   twoclass, weights  # shortcut tuning instance at$tuning_instance #> <TuningInstanceSingleCrit> #> * State:  Optimized #> * Objective: <ObjectiveTuning:classif.rpart_on_pima> #> * Search Space: #> <ParamSet> #>    id    class    lower     upper nlevels        default value #> 1: cp ParamDbl -9.21034 -2.302585     Inf <NoDefault[3]>       #> Trafo is set. #> * Terminator: <TerminatorEvals> #> * Terminated: TRUE #> * Result: #>           cp learner_param_vals  x_domain classif.ce #> 1: -8.058762          <list[2]> <list[1]>  0.2780488 #> * Archive: #> <ArchiveTuning> #>      cp classif.ce runtime_learners           timestamp batch_nr #> 1: -8.1       0.28            0.029 2021-12-26 04:26:21        1 #> 2: -4.2       0.28            0.076 2021-12-26 04:26:21        2 #> 3: -6.9       0.28            0.016 2021-12-26 04:26:21        3 #> 4: -7.9       0.28            0.015 2021-12-26 04:26:21        4 #> 5: -6.9       0.28            0.017 2021-12-26 04:26:21        5 #>         resample_result #> 1: <ResampleResult[22]> #> 2: <ResampleResult[22]> #> 3: <ResampleResult[22]> #> 4: <ResampleResult[22]> #> 5: <ResampleResult[22]>   ### nested resampling  at = AutoTuner$new(   learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   terminator = trm(\"evals\", n_evals = 5),   tuner = tnr(\"random_search\"))  resampling_outer = rsmp(\"cv\", folds = 3) rr = resample(task, at, resampling_outer, store_models = TRUE)  # retrieve inner tuning results. extract_inner_tuning_results(rr) #>    iteration        cp classif.ce learner_param_vals  x_domain task_id #> 1:         1 -8.375592  0.2982456          <list[2]> <list[1]>    pima #> 2:         2 -6.714384  0.3040936          <list[2]> <list[1]>    pima #> 3:         3 -4.608131  0.2631579          <list[2]> <list[1]>    pima #>             learner_id resampling_id #> 1: classif.rpart.tuned            cv #> 2: classif.rpart.tuned            cv #> 3: classif.rpart.tuned            cv  # performance scores estimated on the outer resampling rr$score() #>                 task task_id         learner          learner_id #> 1: <TaskClassif[49]>    pima <AutoTuner[41]> classif.rpart.tuned #> 2: <TaskClassif[49]>    pima <AutoTuner[41]> classif.rpart.tuned #> 3: <TaskClassif[49]>    pima <AutoTuner[41]> classif.rpart.tuned #>            resampling resampling_id iteration              prediction #> 1: <ResamplingCV[19]>            cv         1 <PredictionClassif[20]> #> 2: <ResamplingCV[19]>            cv         2 <PredictionClassif[20]> #> 3: <ResamplingCV[19]>            cv         3 <PredictionClassif[20]> #>    classif.ce #> 1:  0.2773438 #> 2:  0.2968750 #> 3:  0.1992188  # unbiased performance of the final model trained on the full data set rr$aggregate() #> classif.ce  #>  0.2578125"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":null,"dir":"Reference","previous_headings":"","what":"ObjectiveTuning â ObjectiveTuning","title":"ObjectiveTuning â ObjectiveTuning","text":"Stores objective function estimates performance hyperparameter configurations. class usually constructed internally TuningInstanceSingleCrit / TuningInstanceMultiCrit.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"ObjectiveTuning â ObjectiveTuning","text":"bbotk::Objective -> ObjectiveTuning","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"ObjectiveTuning â ObjectiveTuning","text":"task (mlr3::Task). learner (mlr3::Learner). resampling (mlr3::Resampling). measures (list mlr3::Measure). store_models (logical(1)). store_benchmark_result (logical(1)). archive (ArchiveTuning). hotstart_stack (mlr3::HotstartStack). allow_hotstart (logical(1)).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"ObjectiveTuning â ObjectiveTuning","text":"bbotk::Objective$eval() bbotk::Objective$eval_dt() bbotk::Objective$eval_many() bbotk::Objective$format() bbotk::Objective$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"ObjectiveTuning â ObjectiveTuning","text":"ObjectiveTuning$new() ObjectiveTuning$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"ObjectiveTuning â ObjectiveTuning","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ObjectiveTuning â ObjectiveTuning","text":"","code":"ObjectiveTuning$new(   task,   learner,   resampling,   measures,   store_benchmark_result = TRUE,   store_models = FALSE,   check_values = TRUE,   allow_hotstart = FALSE,   archive = NULL )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ObjectiveTuning â ObjectiveTuning","text":"task (mlr3::Task) Task operate . learner (mlr3::Learner) Learner tune. resampling (mlr3::Resampling) Resampling used evaluated performance hyperparameter configurations. Uninstantiated resamplings instantiated construction configurations evaluated data splits. Already instantiated resamplings kept unchanged. Specialized Tuner change resampling e.g. evaluate hyperparameter configuration different data splits. field, however, always returns resampling passed construction. measures (list mlr3::Measure) Measures optimize. store_benchmark_result (logical(1)) TRUE (default), store resample result evaluated hyperparameter configurations archive mlr3::BenchmarkResult. store_models (logical(1)) TRUE, fitted models stored benchmark result (archive$benchmark_result). store_benchmark_result = FALSE, models stored temporarily accessible tuning. combination needed measures require model. check_values (logical(1)) TRUE, hyperparameter values checked evaluation performance scores . FALSE (default), values unchecked computational overhead reduced. allow_hotstart (logical(1)) Allow hotstart learners previously fitted models. See also mlr3::HotstartStack. learner must support hotstarting. Sets store_models = TRUE. archive (ArchiveTuning) Reference archive TuningInstanceSingleCrit | TuningInstanceMultiCrit. NULL (default), benchmark result models stored.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"ObjectiveTuning â ObjectiveTuning","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"ObjectiveTuning â ObjectiveTuning","text":"","code":"ObjectiveTuning$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/ObjectiveTuning.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"ObjectiveTuning â ObjectiveTuning","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":null,"dir":"Reference","previous_headings":"","what":"Tuner â Tuner","title":"Tuner â Tuner","text":"Abstract Tuner class implements base functionality tuner must provide. tuner object describes tuning strategy, .e. optimize black-box function feasible set defined TuningInstanceSingleCrit / TuningInstanceMultiCrit object. tuner must write result TuningInstanceSingleCrit / TuningInstanceMultiCrit using assign_result method bbotk::OptimInstance end tuning order store best selected hyperparameter configuration estimated performance vector.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"private-methods","dir":"Reference","previous_headings":"","what":"Private Methods","title":"Tuner â Tuner","text":".optimize(instance) -> NULL Abstract base method. Implement specify tuning subclass. See technical details sections. .assign_result(instance) -> NULL Abstract base method. Implement specify final configuration selected. See technical details sections.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"technical-details-and-subclasses","dir":"Reference","previous_headings":"","what":"Technical Details and Subclasses","title":"Tuner â Tuner","text":"subclass implemented following way: Inherit Tuner. Specify private abstract method $.tune() use call optimizer. need call instance$eval_batch() evaluate design points. batch evaluation requested TuningInstanceSingleCrit / TuningInstanceMultiCrit object instance, batch possibly executed parallel via mlr3::benchmark(), evaluations stored inside instance$archive. batch evaluation, bbotk::Terminator checked, positive, exception class \"terminated_error\" generated. later case current batch evaluations still stored instance, numeric scores sent back handling optimizer lost execution control. exception caught select best configuration instance$archive return . Note therefore points specified bbotk::Terminator may evaluated, Terminator checked batch evaluation, -evaluation batch. many depends setting batch size. Overwrite private super-method .assign_result() want decide estimate final configuration instance estimated performance. default behavior : pick best resample-experiment, regarding given measure, assign configuration aggregated performance instance.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Tuner â Tuner","text":"param_set (paradox::ParamSet). param_classes (character()). properties (character()). packages (character()).","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Tuner â Tuner","text":"Tuner$new() Tuner$format() Tuner$print() Tuner$optimize() Tuner$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Tuner â Tuner","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuner â Tuner","text":"","code":"Tuner$new(param_set, param_classes, properties, packages = character())"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tuner â Tuner","text":"param_set (paradox::ParamSet) Set control parameters tuner. param_classes (character()) Supported parameter classes learner hyperparameters tuner can optimize, subclasses paradox::Param. properties (character()) Set properties tuner. Must subset mlr_reflections$tuner_properties. packages (character()) Set required packages. Note packages loaded via requireNamespace(), attached.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"method-format-","dir":"Reference","previous_headings":"","what":"Method format()","title":"Tuner â Tuner","text":"Helper print outputs.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuner â Tuner","text":"","code":"Tuner$format()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Tuner â Tuner","text":"Print method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuner â Tuner","text":"","code":"Tuner$print()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Tuner â Tuner","text":"(character()).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"method-optimize-","dir":"Reference","previous_headings":"","what":"Method optimize()","title":"Tuner â Tuner","text":"Performs tuning TuningInstanceSingleCrit TuningInstanceMultiCrit termination. single evaluations written ArchiveTuning resides TuningInstanceSingleCrit/TuningInstanceMultiCrit. result written instance object.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuner â Tuner","text":"","code":"Tuner$optimize(inst)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tuner â Tuner","text":"inst (TuningInstanceSingleCrit | TuningInstanceMultiCrit).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Tuner â Tuner","text":"data.table::data.table","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Tuner â Tuner","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuner â Tuner","text":"","code":"Tuner$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tuner â Tuner","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/Tuner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tuner â Tuner","text":"","code":"instance = TuningInstanceSingleCrit$new(   task = tsk(\"iris\"),   learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   terminator = trm(\"evals\", n_evals = 3) ) tuner = tnr(\"random_search\")  # optimize hyperparameter # modifies the instance by reference tuner$optimize(instance) #>          cp learner_param_vals  x_domain classif.ce #> 1: -7.72638          <list[2]> <list[1]>       0.02  # returns best configuration and best performance instance$result #>          cp learner_param_vals  x_domain classif.ce #> 1: -7.72638          <list[2]> <list[1]>       0.02  # allows access of data.table of full path of all evaluations instance$archive #> <ArchiveTuning> #>      cp classif.ce runtime_learners           timestamp batch_nr #> 1: -7.7       0.02            0.027 2021-12-26 04:26:25        1 #> 2: -8.4       0.02            0.010 2021-12-26 04:26:25        2 #> 3: -7.4       0.02            0.008 2021-12-26 04:26:25        3 #>         resample_result #> 1: <ResampleResult[22]> #> 2: <ResampleResult[22]> #> 3: <ResampleResult[22]>"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":null,"dir":"Reference","previous_headings":"","what":"TunerFromOptimizer â TunerFromOptimizer","title":"TunerFromOptimizer â TunerFromOptimizer","text":"Internally used transform bbotk::Optimizer Tuner.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"TunerFromOptimizer â TunerFromOptimizer","text":"mlr3tuning::Tuner -> TunerFromOptimizer","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"TunerFromOptimizer â TunerFromOptimizer","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"TunerFromOptimizer â TunerFromOptimizer","text":"TunerFromOptimizer$new() TunerFromOptimizer$optimize() TunerFromOptimizer$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"TunerFromOptimizer â TunerFromOptimizer","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"TunerFromOptimizer â TunerFromOptimizer","text":"","code":"TunerFromOptimizer$new(optimizer)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"TunerFromOptimizer â TunerFromOptimizer","text":"optimizer bbotk::Optimizer Optimizer called.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"method-optimize-","dir":"Reference","previous_headings":"","what":"Method optimize()","title":"TunerFromOptimizer â TunerFromOptimizer","text":"Performs tuning TuningInstanceSingleCrit / TuningInstanceMultiCrit termination. single evaluations final results written ArchiveTuning resides TuningInstanceSingleCrit/TuningInstanceMultiCrit. final result returned.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"TunerFromOptimizer â TunerFromOptimizer","text":"","code":"TunerFromOptimizer$optimize(inst)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"TunerFromOptimizer â TunerFromOptimizer","text":"inst (TuningInstanceSingleCrit | TuningInstanceMultiCrit).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"TunerFromOptimizer â TunerFromOptimizer","text":"data.table::data.table.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"TunerFromOptimizer â TunerFromOptimizer","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"TunerFromOptimizer â TunerFromOptimizer","text":"","code":"TunerFromOptimizer$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TunerFromOptimizer.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"TunerFromOptimizer â TunerFromOptimizer","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":null,"dir":"Reference","previous_headings":"","what":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"Specifies general multi-criteria tuning scenario, including objective function archive Tuners act upon. class stores ObjectiveTuning object encodes black box objective function Tuner optimize. allows basic operations querying objective design points ($eval_batch()), storing evaluations internal Archive accessing final result ($result). Evaluations hyperparameter configurations performed batches calling mlr3::benchmark() internally. batch evaluated, bbotk::Terminator queried remaining budget. available budget exhausted, exception raised, evaluations can performed point . tuner also supposed store final result, consisting selected hyperparameter configuration associated estimated performance values, calling method instance$assign_result.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"bbotk::OptimInstance -> bbotk::OptimInstanceMultiCrit -> TuningInstanceMultiCrit","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"result_learner_param_vals (list()) List param values optimal learner call.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"bbotk::OptimInstance$clear() bbotk::OptimInstance$eval_batch() bbotk::OptimInstance$format() bbotk::OptimInstance$objective_function() bbotk::OptimInstance$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"TuningInstanceMultiCrit$new() TuningInstanceMultiCrit$assign_result() TuningInstanceMultiCrit$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"Creates new instance R6 class. defines resampled performance learner task, feasibility region parameters tuner supposed optimize, termination criterion.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"","code":"TuningInstanceMultiCrit$new(   task,   learner,   resampling,   measures,   terminator,   search_space = NULL,   store_benchmark_result = TRUE,   store_models = FALSE,   check_values = FALSE,   allow_hotstart = FALSE )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"task (mlr3::Task) Task operate . learner (mlr3::Learner) Learner tune. resampling (mlr3::Resampling) Resampling used evaluated performance hyperparameter configurations. Uninstantiated resamplings instantiated construction configurations evaluated data splits. Already instantiated resamplings kept unchanged. Specialized Tuner change resampling e.g. evaluate hyperparameter configuration different data splits. field, however, always returns resampling passed construction. measures (list mlr3::Measure) Measures optimize. terminator (Terminator) Stop criterion tuning process. search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). store_benchmark_result (logical(1)) TRUE (default), store resample result evaluated hyperparameter configurations archive mlr3::BenchmarkResult. store_models (logical(1)) TRUE, fitted models stored benchmark result (archive$benchmark_result). store_benchmark_result = FALSE, models stored temporarily accessible tuning. combination needed measures require model. check_values (logical(1)) TRUE, hyperparameter values checked evaluation performance scores . FALSE (default), values unchecked computational overhead reduced. allow_hotstart (logical(1)) Allow hotstart learners previously fitted models. See also mlr3::HotstartStack. learner must support hotstarting. Sets store_models = TRUE.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"method-assign-result-","dir":"Reference","previous_headings":"","what":"Method assign_result()","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"Tuner object writes best found points estimated performance values . internal use.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"","code":"TuningInstanceMultiCrit$assign_result(xdt, ydt, learner_param_vals = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"xdt (data.table::data.table()) Hyperparameter values data.table::data.table(). row one configuration. Contains values search space. Can contain additional columns extra information. ydt (data.table::data.table()) Optimal outcomes, e.g. Pareto front. learner_param_vals (List named list()s) Fixed parameter values learner neither part ","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"","code":"TuningInstanceMultiCrit$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceMultiCrit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multi Criteria Tuning Instance â TuningInstanceMultiCrit","text":"","code":"library(data.table)  # define search space search_space = ps(   cp = p_dbl(lower = 0.001, upper = 0.1),   minsplit = p_int(lower = 1, upper = 10) )  # initialize instance instance = TuningInstanceMultiCrit$new(   task = tsk(\"iris\"),   learner = lrn(\"classif.rpart\"),   resampling = rsmp(\"holdout\"),   measure = msrs(c(\"classif.ce\", \"classif.acc\")),   search_space = search_space,   terminator = trm(\"evals\", n_evals = 5) )  # generate design design = data.table(cp = c(0.05, 0.01), minsplit = c(5, 3))  # eval design instance$eval_batch(design)  # show archive instance$archive #> <ArchiveTuning> #>      cp minsplit classif.ce classif.acc runtime_learners           timestamp #> 1: 0.05        5       0.02        0.98            0.008 2021-12-26 04:26:26 #> 2: 0.01        3       0.02        0.98            0.008 2021-12-26 04:26:26 #>    batch_nr      resample_result #> 1:        1 <ResampleResult[22]> #> 2:        1 <ResampleResult[22]>"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":null,"dir":"Reference","previous_headings":"","what":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"Specifies general single-criteria tuning scenario, including objective function archive Tuners act upon. class stores ObjectiveTuning object encodes black box objective function Tuner optimize. allows basic operations querying objective design points ($eval_batch()), storing evaluations internal ArchiveTuning accessing final result ($result). Evaluations hyperparameter configurations performed batches calling mlr3::benchmark() internally. batch evaluated, bbotk::Terminator queried remaining budget. available budget exhausted, exception raised, evaluations can performed point . tuner also supposed store final result, consisting selected hyperparameter configuration associated estimated performance values, calling method instance$assign_result.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"bbotk::OptimInstance -> bbotk::OptimInstanceSingleCrit -> TuningInstanceSingleCrit","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"active-bindings","dir":"Reference","previous_headings":"","what":"Active bindings","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"result_learner_param_vals (list()) Param values optimal learner call.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"bbotk::OptimInstance$clear() bbotk::OptimInstance$eval_batch() bbotk::OptimInstance$format() bbotk::OptimInstance$objective_function() bbotk::OptimInstance$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"TuningInstanceSingleCrit$new() TuningInstanceSingleCrit$assign_result() TuningInstanceSingleCrit$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"Creates new instance R6 class. defines resampled performance learner task, feasibility region parameters tuner supposed optimize, termination criterion.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"","code":"TuningInstanceSingleCrit$new(   task,   learner,   resampling,   measure,   terminator,   search_space = NULL,   store_benchmark_result = TRUE,   store_models = FALSE,   check_values = FALSE,   allow_hotstart = FALSE )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"task (mlr3::Task) Task operate . learner (mlr3::Learner) Learner tune. resampling (mlr3::Resampling) Resampling used evaluated performance hyperparameter configurations. Uninstantiated resamplings instantiated construction configurations evaluated data splits. Already instantiated resamplings kept unchanged. Specialized Tuner change resampling e.g. evaluate hyperparameter configuration different data splits. field, however, always returns resampling passed construction. measure (mlr3::Measure) Measure optimize. terminator (Terminator) Stop criterion tuning process. search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). store_benchmark_result (logical(1)) TRUE (default), store resample result evaluated hyperparameter configurations archive mlr3::BenchmarkResult. store_models (logical(1)) TRUE, fitted models stored benchmark result (archive$benchmark_result). store_benchmark_result = FALSE, models stored temporarily accessible tuning. combination needed measures require model. check_values (logical(1)) TRUE, hyperparameter values checked evaluation performance scores . FALSE (default), values unchecked computational overhead reduced. allow_hotstart (logical(1)) Allow hotstart learners previously fitted models. See also mlr3::HotstartStack. learner must support hotstarting. Sets store_models = TRUE.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"method-assign-result-","dir":"Reference","previous_headings":"","what":"Method assign_result()","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"Tuner object writes best found point estimated performance value . internal use.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"","code":"TuningInstanceSingleCrit$assign_result(xdt, y, learner_param_vals = NULL)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"xdt (data.table::data.table()) Hyperparameter values data.table::data.table(). row one configuration. Contains values search space. Can contain additional columns extra information. y (numeric(1)) Optimal outcome. learner_param_vals (List named list()s) Fixed parameter values learner neither part ","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"","code":"TuningInstanceSingleCrit$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/TuningInstanceSingleCrit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Single Criterion Tuning Instance â TuningInstanceSingleCrit","text":"","code":"library(data.table)  # define search space search_space = ps(   cp = p_dbl(lower = 0.001, upper = 0.1),   minsplit = p_int(lower = 1, upper = 10) )  # initialize instance instance = TuningInstanceSingleCrit$new(   task = tsk(\"iris\"),   learner = lrn(\"classif.rpart\"),   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   search_space = search_space,   terminator = trm(\"evals\", n_evals = 5) )  # generate design design = data.table(cp = c(0.05, 0.01), minsplit = c(5, 3))  # eval design instance$eval_batch(design)  # show archive instance$archive #> <ArchiveTuning> #>      cp minsplit classif.ce runtime_learners           timestamp batch_nr #> 1: 0.05        5       0.04            0.007 2021-12-26 04:26:27        1 #> 2: 0.01        3       0.02            0.009 2021-12-26 04:26:27        1 #>         resample_result #> 1: <ResampleResult[22]> #> 2: <ResampleResult[22]>  ### error handling  # get a learner which breaks with 50% probability # set encapsulation + fallback learner = lrn(\"classif.debug\", error_train = 0.5) learner$encapsulate = c(train = \"evaluate\", predict = \"evaluate\") learner$fallback = lrn(\"classif.featureless\")  # define search space search_space = ps(  x = p_dbl(lower = 0, upper = 1) )  instance = TuningInstanceSingleCrit$new(   task = tsk(\"wine\"),   learner = learner,   resampling = rsmp(\"cv\", folds = 3),   measure = msr(\"classif.ce\"),   search_space = search_space,   terminator = trm(\"evals\", n_evals = 5) )  instance$eval_batch(data.table(x = 1:5 / 5))"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/auto_tuner.html","id":null,"dir":"Reference","previous_headings":"","what":"Syntactic Sugar for Automatic Tuning â auto_tuner","title":"Syntactic Sugar for Automatic Tuning â auto_tuner","text":"Function create AutoTuner object.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/auto_tuner.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Syntactic Sugar for Automatic Tuning â auto_tuner","text":"","code":"auto_tuner(   method,   learner,   resampling,   measure,   term_evals = NULL,   term_time = NULL,   search_space = NULL,   store_models = FALSE,   ... )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/auto_tuner.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Syntactic Sugar for Automatic Tuning â auto_tuner","text":"method (character(1)) Key retrieve tuner mlr_tuners dictionary. learner (mlr3::Learner) Learner tune. resampling (mlr3::Resampling) Resampling used evaluated performance hyperparameter configurations. Uninstantiated resamplings instantiated construction configurations evaluated data splits. Already instantiated resamplings kept unchanged. Specialized Tuner change resampling e.g. evaluate hyperparameter configuration different data splits. field, however, always returns resampling passed construction. measure (mlr3::Measure) Measure optimize. term_evals (integer(1)) Number allowed evaluations. term_time (integer(1)) Maximum allowed time seconds. search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). store_models (logical(1)) TRUE, fitted models stored benchmark result (archive$benchmark_result). store_benchmark_result = FALSE, models stored temporarily accessible tuning. combination needed measures require model. ... (named list()) Named arguments set parameters tuner.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/auto_tuner.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Syntactic Sugar for Automatic Tuning â auto_tuner","text":"AutoTuner","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/auto_tuner.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Syntactic Sugar for Automatic Tuning â auto_tuner","text":"","code":"at = auto_tuner(   method = \"random_search\",   learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),   resampling = rsmp (\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 4)  at$train(tsk(\"pima\"))"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_archives.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Inner Tuning Archives â extract_inner_tuning_archives","title":"Extract Inner Tuning Archives â extract_inner_tuning_archives","text":"Extract inner tuning archives nested resampling. Implemented mlr3::ResampleResult mlr3::BenchmarkResult. function iterates AutoTuner objects binds tuning archives data.table::data.table(). AutoTuner must initialized store_tuning_instance = TRUE resample() benchmark() must called store_models = TRUE.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_archives.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Inner Tuning Archives â extract_inner_tuning_archives","text":"","code":"extract_inner_tuning_archives(   x,   unnest = \"x_domain\",   exclude_columns = \"uhash\" )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_archives.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Inner Tuning Archives â extract_inner_tuning_archives","text":"x (mlr3::ResampleResult | mlr3::BenchmarkResult). unnest (character()) Transforms list columns separate columns. default, x_domain unnested. Set NULL column unnested. exclude_columns (character()) Exclude columns result table. Set NULL column excluded.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_archives.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Inner Tuning Archives â extract_inner_tuning_archives","text":"data.table::data.table().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_archives.html","id":"data-structure","dir":"Reference","previous_headings":"","what":"Data structure","title":"Extract Inner Tuning Archives â extract_inner_tuning_archives","text":"returned data table following columns: experiment (integer(1)) Index, giving according row number original benchmark grid. iteration (integer(1)) Iteration outer resampling. One column hyperparameter search spaces. One column performance measure. runtime_learners (numeric(1)) Sum training predict times logged learners per mlr3::ResampleResult / evaluation. include potential overhead time. timestamp (POSIXct) Time stamp evaluation logged archive. batch_nr (integer(1)) Hyperparameters evaluated batches. batch unique batch number. x_domain (list()) List transformed hyperparameter values. default column unnested. x_domain_* () Separate column transformed hyperparameter. resample_result (mlr3::ResampleResult) Resample result inner resampling. task_id (character(1)). learner_id (character(1)). resampling_id (character(1)).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_archives.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Inner Tuning Archives â extract_inner_tuning_archives","text":"","code":"learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE))  at = auto_tuner(   method = \"grid_search\",   learner = learner,   resampling = rsmp (\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 4)  resampling_outer = rsmp(\"cv\", folds = 2) rr = resample(tsk(\"iris\"), at, resampling_outer, store_models = TRUE)  extract_inner_tuning_archives(rr) #>    iteration        cp classif.ce  x_domain_cp runtime_learners #> 1:         1 -8.442812          0 0.0002154435            0.009 #> 2:         1 -5.372699          0 0.0046415888            0.008 #> 3:         1 -7.675284          0 0.0004641589            0.010 #> 4:         1 -3.070113          0 0.0464158883            0.009 #> 5:         2 -4.605170          0 0.0100000000            0.010 #> 6:         2 -3.070113          0 0.0464158883            0.008 #> 7:         2 -7.675284          0 0.0004641589            0.009 #> 8:         2 -3.837642          0 0.0215443469            0.009 #>              timestamp batch_nr      resample_result task_id #> 1: 2021-12-26 04:26:30        1 <ResampleResult[22]>    iris #> 2: 2021-12-26 04:26:30        2 <ResampleResult[22]>    iris #> 3: 2021-12-26 04:26:31        3 <ResampleResult[22]>    iris #> 4: 2021-12-26 04:26:31        4 <ResampleResult[22]>    iris #> 5: 2021-12-26 04:26:31        1 <ResampleResult[22]>    iris #> 6: 2021-12-26 04:26:31        2 <ResampleResult[22]>    iris #> 7: 2021-12-26 04:26:31        3 <ResampleResult[22]>    iris #> 8: 2021-12-26 04:26:31        4 <ResampleResult[22]>    iris #>             learner_id resampling_id #> 1: classif.rpart.tuned            cv #> 2: classif.rpart.tuned            cv #> 3: classif.rpart.tuned            cv #> 4: classif.rpart.tuned            cv #> 5: classif.rpart.tuned            cv #> 6: classif.rpart.tuned            cv #> 7: classif.rpart.tuned            cv #> 8: classif.rpart.tuned            cv"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Inner Tuning Results â extract_inner_tuning_results","title":"Extract Inner Tuning Results â extract_inner_tuning_results","text":"Extract inner tuning results nested resampling. Implemented mlr3::ResampleResult mlr3::BenchmarkResult. function iterates AutoTuner objects binds tuning results data.table::data.table(). AutoTuner must initialized store_tuning_instance = TRUE resample() benchmark() must called store_models = TRUE.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Inner Tuning Results â extract_inner_tuning_results","text":"","code":"extract_inner_tuning_results(x)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Inner Tuning Results â extract_inner_tuning_results","text":"x (mlr3::ResampleResult | mlr3::BenchmarkResult).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Inner Tuning Results â extract_inner_tuning_results","text":"data.table::data.table().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_results.html","id":"data-structure","dir":"Reference","previous_headings":"","what":"Data structure","title":"Extract Inner Tuning Results â extract_inner_tuning_results","text":"returned data table following columns: experiment (integer(1)) Index, giving according row number original benchmark grid. iteration (integer(1)) Iteration outer resampling. One column hyperparameter search spaces. One column performance measure. learner_param_vals (list()) Hyperparameter values used learner. Includes fixed proposed hyperparameter values. x_domain (list()) List transformed hyperparameter values. task_id (character(1)). learner_id (character(1)). resampling_id (character(1)).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/extract_inner_tuning_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Inner Tuning Results â extract_inner_tuning_results","text":"","code":"learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE))  at = auto_tuner(   method = \"grid_search\",   learner = learner,   resampling = rsmp (\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 4)  resampling_outer = rsmp(\"cv\", folds = 2) rr = resample(tsk(\"iris\"), at, resampling_outer, store_models = TRUE)  extract_inner_tuning_results(rr) #>    iteration        cp classif.ce learner_param_vals  x_domain task_id #> 1:         1 -8.442812       0.04          <list[2]> <list[1]>    iris #> 2:         2 -9.210340       0.04          <list[2]> <list[1]>    iris #>             learner_id resampling_id #> 1: classif.rpart.tuned            cv #> 2: classif.rpart.tuned            cv"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr3tuning-package.html","id":null,"dir":"Reference","previous_headings":"","what":"mlr3tuning: Tuning for 'mlr3' â mlr3tuning-package","title":"mlr3tuning: Tuning for 'mlr3' â mlr3tuning-package","text":"Implements methods hyperparameter tuning 'mlr3', e.g. grid search, random search, generalized simulated annealing iterated racing. Various termination criteria can set combined. class 'AutoTuner' provides convenient way perform nested resampling combination 'mlr3'.","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr3tuning-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"mlr3tuning: Tuning for 'mlr3' â mlr3tuning-package","text":"Maintainer: Marc Becker marcbecker@posteo.de (ORCID) Authors: Michel Lang michellang@gmail.com (ORCID) Jakob Richter jakob1richter@gmail.com (ORCID) Bernd Bischl bernd_bischl@gmx.net (ORCID) Daniel Schalk daniel.schalk@stat.uni-muenchen.de (ORCID)","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners.html","id":null,"dir":"Reference","previous_headings":"","what":"Dictionary of Tuners â mlr_tuners","title":"Dictionary of Tuners â mlr_tuners","text":"simple mlr3misc::Dictionary storing objects class Tuner. tuner associated help page, see mlr_tuners_[id]. dictionary can get populated additional tuners add-packages. convenient way retrieve construct tuner, see tnr()/tnrs().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dictionary of Tuners â mlr_tuners","text":"","code":"mlr_tuners"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Dictionary of Tuners â mlr_tuners","text":"R6::R6Class object inheriting mlr3misc::Dictionary.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Dictionary of Tuners â mlr_tuners","text":"See mlr3misc::Dictionary.","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Dictionary of Tuners â mlr_tuners","text":"","code":"mlr_tuners$get(\"grid_search\") #> <TunerGridSearch> #> * Parameters: resolution=10, batch_size=1 #> * Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct #> * Properties: dependencies, single-crit, multi-crit #> * Packages: mlr3tuning tnr(\"random_search\") #> <TunerRandomSearch> #> * Parameters: batch_size=1 #> * Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct #> * Properties: dependencies, single-crit, multi-crit #> * Packages: mlr3tuning, bbotk"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"Subclass implements CMA-ES calling adagio::pureCMAES() package adagio.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"Hansen N (2016). âCMA Evolution Strategy: Tutorial.â 1604.00772.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"Tuner can instantiated via dictionary mlr_tuners associated sugar function tnr():","code":"TunerCmaes$new() mlr_tuners$get(\"cmaes\") tnr(\"cmaes\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"sigma numeric(1) start_values character(1) Create random start values based center search space? latter case, center parameters trafo applied. meaning control parameters, see adagio::pureCMAES(). Note removed control parameters refer termination algorithm terminators allow obtain behavior.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"mlr3tuning::Tuner -> mlr3tuning::TunerFromOptimizer -> TunerCmaes","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$print() mlr3tuning::TunerFromOptimizer$optimize()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"TunerCmaes$new() TunerCmaes$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"","code":"TunerCmaes$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"","code":"TunerCmaes$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_cmaes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter Tuning with Covariance Matrix Adaptation Evolution Strategy â mlr_tuners_cmaes","text":"","code":"library(data.table)  # retrieve task task = tsk(\"pima\")  # load learner and set search space learner = lrn(\"classif.rpart\",    cp = to_tune(1e-04, 1e-1, logscale = TRUE),    minsplit = to_tune(p_dbl(2, 128, trafo = as.integer)),   minbucket = to_tune(p_dbl(1, 64, trafo = as.integer)) )  # hyperparameter tuning on the pima indians diabetes data set instance = tune(   method = \"cmaes\",   task = task,   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 10)  # best performing hyperparameter configuration instance$result #>           cp minsplit minbucket learner_param_vals  x_domain classif.ce #> 1: -6.176698  23.9133  55.01354          <list[4]> <list[3]>   0.234375  # all evaluated hyperparameter configuration as.data.table(instance$archive) #>            cp  minsplit minbucket classif.ce  x_domain_cp x_domain_minsplit #>  1: -6.176698  23.91330  55.01354  0.2343750 0.0020772764                23 #>  2: -2.302585  50.89360  64.00000  0.2773438 0.1000000000                50 #>  3: -2.302585  37.66173  64.00000  0.2773438 0.1000000000                37 #>  4: -2.302585 128.00000  64.00000  0.2773438 0.1000000000               128 #>  5: -5.191562 128.00000  64.00000  0.2812500 0.0055633112               128 #>  6: -7.618326 106.36456  64.00000  0.2812500 0.0004913639               106 #>  7: -4.533385 128.00000  64.00000  0.2812500 0.0107442446               128 #>  8: -3.901800  15.77507  55.33608  0.2343750 0.0202055118                15 #>  9: -7.837845 114.29484  27.37630  0.2773438 0.0003945185               114 #> 10: -2.302585 128.00000  64.00000  0.2773438 0.1000000000               128 #>     x_domain_minbucket runtime_learners           timestamp batch_nr #>  1:                 55            0.017 2021-12-26 04:26:34        1 #>  2:                 64            0.020 2021-12-26 04:26:35        2 #>  3:                 64            0.015 2021-12-26 04:26:35        3 #>  4:                 64            0.014 2021-12-26 04:26:35        4 #>  5:                 64            0.014 2021-12-26 04:26:35        5 #>  6:                 64            0.014 2021-12-26 04:26:35        6 #>  7:                 64            0.014 2021-12-26 04:26:35        7 #>  8:                 55            0.015 2021-12-26 04:26:35        8 #>  9:                 27            0.015 2021-12-26 04:26:35        9 #> 10:                 64            0.014 2021-12-26 04:26:35       10 #>          resample_result #>  1: <ResampleResult[22]> #>  2: <ResampleResult[22]> #>  3: <ResampleResult[22]> #>  4: <ResampleResult[22]> #>  5: <ResampleResult[22]> #>  6: <ResampleResult[22]> #>  7: <ResampleResult[22]> #>  8: <ResampleResult[22]> #>  9: <ResampleResult[22]> #> 10: <ResampleResult[22]>  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(task)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"Subclass tuning w.r.t. fixed design points. simply search set points fully specified user. points design evaluated order given.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"Tuner can instantiated via dictionary mlr_tuners associated sugar function tnr():","code":"TunerDesignPoints$new() mlr_tuners$get(\"design_points\") tnr(\"design_points\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"parallelization","dir":"Reference","previous_headings":"","what":"Parallelization","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"order support general termination criteria parallelization, evaluate points batch-fashion size batch_size. Larger batches mean can parallelize , smaller batches imply fine-grained checking termination criteria. batch contains batch_size times resampling$iters jobs. E.g., set batch size 10 points 5-fold cross validation, can utilize 50 cores. Parallelization supported via package future (see mlr3::benchmark()'s section parallelization details).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"batch_size integer(1) Maximum number configurations try batch. design data.table::data.table Design points try search, one per row.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"mlr3tuning::Tuner -> mlr3tuning::TunerFromOptimizer -> TunerDesignPoints","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$print() mlr3tuning::TunerFromOptimizer$optimize()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"TunerDesignPoints$new() TunerDesignPoints$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"","code":"TunerDesignPoints$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"","code":"TunerDesignPoints$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_design_points.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter Tuning with via Design Points â mlr_tuners_design_points","text":"","code":"library(data.table)  # retrieve task task = tsk(\"pima\")  # load learner and set search space learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE))  # hyperparameter tuning on the pima indians diabetes data set instance = tune(   method = \"design_points\",   task = task,   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   design = data.table(cp = c(log(1e-1), log(1e-2))) )  # best performing hyperparameter configuration instance$result #>          cp learner_param_vals  x_domain classif.ce #> 1: -4.60517          <list[2]> <list[1]>  0.2421875  # all evaluated hyperparameter configuration as.data.table(instance$archive) #>           cp classif.ce x_domain_cp runtime_learners           timestamp #> 1: -2.302585  0.2851562        0.10            0.015 2021-12-26 04:26:36 #> 2: -4.605170  0.2421875        0.01            0.017 2021-12-26 04:26:36 #>    batch_nr      resample_result #> 1:        1 <ResampleResult[22]> #> 2:        2 <ResampleResult[22]>  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(task)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"Subclass generalized simulated annealing tuning calling GenSA::GenSA() package GenSA.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"Tsallis C, Stariolo DA (1996). âGeneralized simulated annealing.â Physica : Statistical Mechanics Applications, 233(1-2), 395--406. doi: 10.1016/s0378-4371(96)00271-3 . Xiang Y, Gubian S, Suomela B, Hoeng J (2013). âGeneralized Simulated Annealing Global Optimization: GenSA Package.â R Journal, 5(1), 13. doi: 10.32614/rj-2013-002 .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"Tuner can instantiated via dictionary mlr_tuners associated sugar function tnr():","code":"TunerGenSA$new() mlr_tuners$get(\"gensa\") tnr(\"gensa\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"parallelization","dir":"Reference","previous_headings":"","what":"Parallelization","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"order support general termination criteria parallelization, evaluate points batch-fashion size batch_size. Larger batches mean can parallelize , smaller batches imply fine-grained checking termination criteria. batch contains batch_size times resampling$iters jobs. E.g., set batch size 10 points 5-fold cross validation, can utilize 50 cores. Parallelization supported via package future (see mlr3::benchmark()'s section parallelization details).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"smooth logical(1) temperature numeric(1) acceptance.param numeric(1) verbose logical(1) trace.mat logical(1) meaning control parameters, see GenSA::GenSA(). Note removed control parameters refer termination algorithm terminators allow obtain behavior.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"mlr3tuning::Tuner -> mlr3tuning::TunerFromOptimizer -> TunerGenSA","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$print() mlr3tuning::TunerFromOptimizer$optimize()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"TunerGenSA$new() TunerGenSA$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"","code":"TunerGenSA$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"","code":"TunerGenSA$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_gensa.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter Tuning with Generalized Simulated Annealing â mlr_tuners_gensa","text":"","code":"# retrieve task task = tsk(\"pima\")  # load learner and set search space learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE))  # hyperparameter tuning on the pima indians diabetes data set instance = tune(   method = \"gensa\",   task = task,   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 10 )  # best performing hyperparameter configuration instance$result #>           cp learner_param_vals  x_domain classif.ce #> 1: -2.389247          <list[2]> <list[1]>  0.2382812  # all evaluated hyperparameter configuration as.data.table(instance$archive) #>            cp classif.ce  x_domain_cp runtime_learners           timestamp #>  1: -4.018507  0.2500000 0.0179797966            0.015 2021-12-26 04:26:37 #>  2: -8.055934  0.2656250 0.0003172140            0.017 2021-12-26 04:26:37 #>  3: -5.866459  0.2539062 0.0028328858            0.017 2021-12-26 04:26:37 #>  4: -4.018507  0.2500000 0.0179797966            0.015 2021-12-26 04:26:37 #>  5: -4.018506  0.2500000 0.0179798146            0.017 2021-12-26 04:26:37 #>  6: -4.018508  0.2500000 0.0179797786            0.018 2021-12-26 04:26:38 #>  7: -3.940492  0.2500000 0.0194386524            0.016 2021-12-26 04:26:38 #>  8: -4.028445  0.2500000 0.0178019864            0.017 2021-12-26 04:26:38 #>  9: -2.389247  0.2382812 0.0916987051            0.015 2021-12-26 04:26:38 #> 10: -7.449061  0.2656250 0.0005819876            0.017 2021-12-26 04:26:38 #>     batch_nr      resample_result #>  1:        1 <ResampleResult[22]> #>  2:        2 <ResampleResult[22]> #>  3:        3 <ResampleResult[22]> #>  4:        4 <ResampleResult[22]> #>  5:        5 <ResampleResult[22]> #>  6:        6 <ResampleResult[22]> #>  7:        7 <ResampleResult[22]> #>  8:        8 <ResampleResult[22]> #>  9:        9 <ResampleResult[22]> #> 10:       10 <ResampleResult[22]>  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(task)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"Subclass grid search tuning. grid constructed Cartesian product discretized values per parameter, see paradox::generate_design_grid(). learner supports hotstarting, grid sorted hotstart parameter (see also mlr3::HotstartStack). , points grid evaluated random order.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"Tuner can instantiated via dictionary mlr_tuners associated sugar function tnr():","code":"TunerGridSearch$new() mlr_tuners$get(\"grid_search\") tnr(\"grid_search\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"resolution integer(1) Resolution grid, see paradox::generate_design_grid(). param_resolutions named integer() Resolution per parameter, named parameter ID, see paradox::generate_design_grid(). batch_size integer(1) Maximum number points try batch.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"parallelization","dir":"Reference","previous_headings":"","what":"Parallelization","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"order support general termination criteria parallelization, evaluate points batch-fashion size batch_size. Larger batches mean can parallelize , smaller batches imply fine-grained checking termination criteria. batch contains batch_size times resampling$iters jobs. E.g., set batch size 10 points 5-fold cross validation, can utilize 50 cores. Parallelization supported via package future (see mlr3::benchmark()'s section parallelization details).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"mlr3tuning::Tuner -> TunerGridSearch","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$optimize() mlr3tuning::Tuner$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"TunerGridSearch$new() TunerGridSearch$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"","code":"TunerGridSearch$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"","code":"TunerGridSearch$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_grid_search.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter Tuning with Grid Search â mlr_tuners_grid_search","text":"","code":"# retrieve task task = tsk(\"pima\")  # load learner and set search space learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE))  # hyperparameter tuning on the pima indians diabetes data set instance = tune(   method = \"grid_search\",   task = task,   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 10 )  # best performing hyperparameter configuration instance$result #>           cp learner_param_vals  x_domain classif.ce #> 1: -6.907755          <list[2]> <list[1]>  0.2578125  # all evaluated hyperparameter configuration as.data.table(instance$archive) #>            cp classif.ce  x_domain_cp runtime_learners           timestamp #>  1: -6.907755  0.2578125 0.0010000000            0.032 2021-12-26 04:26:39 #>  2: -3.837642  0.2851562 0.0215443469            0.015 2021-12-26 04:26:39 #>  3: -7.675284  0.2578125 0.0004641589            0.016 2021-12-26 04:26:39 #>  4: -8.442812  0.2578125 0.0002154435            0.034 2021-12-26 04:26:39 #>  5: -5.372699  0.2578125 0.0046415888            0.016 2021-12-26 04:26:39 #>  6: -2.302585  0.2890625 0.1000000000            0.014 2021-12-26 04:26:39 #>  7: -6.140227  0.2578125 0.0021544347            0.016 2021-12-26 04:26:39 #>  8: -9.210340  0.2578125 0.0001000000            0.015 2021-12-26 04:26:39 #>  9: -4.605170  0.2578125 0.0100000000            0.016 2021-12-26 04:26:40 #> 10: -3.070113  0.2773438 0.0464158883            0.031 2021-12-26 04:26:40 #>     batch_nr      resample_result #>  1:        1 <ResampleResult[22]> #>  2:        2 <ResampleResult[22]> #>  3:        3 <ResampleResult[22]> #>  4:        4 <ResampleResult[22]> #>  5:        5 <ResampleResult[22]> #>  6:        6 <ResampleResult[22]> #>  7:        7 <ResampleResult[22]> #>  8:        8 <ResampleResult[22]> #>  9:        9 <ResampleResult[22]> #> 10:       10 <ResampleResult[22]>  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(task)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":null,"dir":"Reference","previous_headings":"","what":"Tuning via Iterated Racing. â mlr_tuners_irace","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"TunerIrace class implements iterated racing. Calls irace::irace() package irace.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"Lopez-Ibanez M, Dubois-Lacoste J, Caceres LP, Birattari M, Stuetzle T (2016). âirace package: Iterated racing automatic algorithm configuration.â Operations Research Perspectives, 3, 43--58. doi: 10.1016/j.orp.2016.09.002 .","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"Tuner can instantiated via dictionary mlr_tuners associated sugar function tnr():","code":"TunerIrace$new() mlr_tuners$get(\"irace\") tnr(\"irace\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"n_instances integer(1) Number resampling instances. meaning parameters, see irace::defaultScenario(). Note removed control parameters refer termination algorithm. Use TerminatorEvals instead. terminators work TunerIrace.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"archive","dir":"Reference","previous_headings":"","what":"Archive","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"ArchiveTuning holds following additional columns: \"race\" (integer(1)) Race iteration. \"step\" (integer(1)) Step number race. \"instance\" (integer(1)) Identifies resampling instances across races steps. \"configuration\" (integer(1)) Identifies configurations across races steps.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"result","dir":"Reference","previous_headings":"","what":"Result","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"tuning result (instance$result) best performing elite final race. reported performance average performance estimated used instances.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"mlr3tuning::Tuner -> mlr3tuning::TunerFromOptimizer -> TunerIrace","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$print()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"TunerIrace$new() TunerIrace$optimize() TunerIrace$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"","code":"TunerIrace$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"method-optimize-","dir":"Reference","previous_headings":"","what":"Method optimize()","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"Performs tuning TuningInstanceSingleCrit termination. single evaluations final results written ArchiveTuning resides TuningInstanceSingleCrit. final result returned.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"","code":"TunerIrace$optimize(inst)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"inst (TuningInstanceSingleCrit).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"data.table::data.table.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"","code":"TunerIrace$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_irace.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Tuning via Iterated Racing. â mlr_tuners_irace","text":"","code":"# retrieve task task = tsk(\"pima\")  # load learner and set search space learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE))  # hyperparameter tuning on the pima indians diabetes data set instance = tune(   method = \"irace\",   task = task,   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 42 ) #> # 2021-12-26 04:26:42 UTC: Initialization #> # Elitist race #> # Elitist new instances: 1 #> # Elitist limit: 2 #> # nbIterations: 2 #> # minNbSurvival: 2 #> # nbParameters: 1 #> # seed: 1322733961 #> # confidence level: 0.95 #> # budget: 42 #> # mu: 5 #> # deterministic: FALSE #>  #> # 2021-12-26 04:26:42 UTC: Iteration 1 of 2 #> # experimentsUsedSoFar: 0 #> # remainingBudget: 42 #> # currentBudget: 21 #> # nbConfigurations: 3 #> # Markers: #>      x No test is performed. #>      c Configurations are discarded only due to capping. #>      - The test is performed and some configurations are discarded. #>      = The test is performed but no configuration is discarded. #>      ! The test is performed and configurations could be discarded but elite configurations are preserved. #>      . All alive configurations are elite and nothing is discarded #>  #> +-+-----------+-----------+-----------+---------------+-----------+--------+-----+----+------+ #> | |   Instance|      Alive|       Best|      Mean best| Exp so far|  W time|  rho|KenW|  Qvar| #> +-+-----------+-----------+-----------+---------------+-----------+--------+-----+----+------+ #> |x|          1|          3|          1|   0.2343750000|          3|00:00:00|   NA|  NA|    NA| #> |x|          2|          3|          1|   0.2207031250|          6|00:00:00|+1.00|1.00|0.1178| #> |x|          3|          3|          1|   0.2408854167|          9|00:00:00|-0.18|0.21|0.8467| #> |x|          4|          3|          1|   0.2333984375|         12|00:00:00|-0.27|0.05|0.8578| #> |=|          5|          3|          1|   0.2390625000|         15|00:00:00|-0.25|0.00|0.8312| #> |=|          6|          3|          2|   0.2441406250|         18|00:00:00|-0.16|0.03|0.7695| #> |=|          7|          3|          2|   0.2438616071|         21|00:00:00|-0.08|0.07|0.7122| #> +-+-----------+-----------+-----------+---------------+-----------+--------+-----+----+------+ #> Best-so-far configuration:           2    mean value:    0.2438616071 #> Description of the best-so-far configuration: #>   .ID.      cp .PARENT. #> 2    2 -3.6055       NA #>  #> # 2021-12-26 04:26:44 UTC: Elite configurations (first number is the configuration ID; listed from best to worst according to the sum of ranks): #>        cp #> 2 -3.6055 #> 3 -2.7227 #> # 2021-12-26 04:26:44 UTC: Iteration 2 of 2 #> # experimentsUsedSoFar: 21 #> # remainingBudget: 21 #> # currentBudget: 21 #> # nbConfigurations: 4 #> # Markers: #>      x No test is performed. #>      c Configurations are discarded only due to capping. #>      - The test is performed and some configurations are discarded. #>      = The test is performed but no configuration is discarded. #>      ! The test is performed and configurations could be discarded but elite configurations are preserved. #>      . All alive configurations are elite and nothing is discarded #>  #> +-+-----------+-----------+-----------+---------------+-----------+--------+-----+----+------+ #> | |   Instance|      Alive|       Best|      Mean best| Exp so far|  W time|  rho|KenW|  Qvar| #> +-+-----------+-----------+-----------+---------------+-----------+--------+-----+----+------+ #> |x|          8|          4|          2|   0.2656250000|          4|00:00:00|   NA|  NA|    NA| #> |x|          7|          4|          2|   0.2539062500|          6|00:00:00|+1.00|1.00|0.0000| #> |x|          6|          4|          2|   0.2500000000|          8|00:00:00|+0.00|0.33|0.2500| #> |x|          2|          4|          2|   0.2539062500|         10|00:00:00|+0.33|0.50|0.2500| #> |!|          1|          4|          2|   0.2531250000|         12|00:00:00|+0.50|0.60|0.2250| #> |!|          3|          4|          2|   0.2545572917|         14|00:00:00|+0.40|0.50|0.2250| #> |=|          4|          4|          4|   0.2472098214|         16|00:00:00|+0.17|0.29|0.3571| #> |=|          5|          4|          4|   0.2441406250|         18|00:00:00|+0.14|0.25|0.3214| #> +-+-----------+-----------+-----------+---------------+-----------+--------+-----+----+------+ #> Best-so-far configuration:           4    mean value:    0.2441406250 #> Description of the best-so-far configuration: #>   .ID.      cp .PARENT. #> 4    4 -3.4067        3 #>  #> # 2021-12-26 04:26:46 UTC: Elite configurations (first number is the configuration ID; listed from best to worst according to the sum of ranks): #>        cp #> 4 -3.4067 #> 5 -3.6850 #> # 2021-12-26 04:26:46 UTC: Stopped because there is not enough budget left to race more than the minimum (2) #> # You may either increase the budget or set 'minNbSurvival' to a lower value #> # Iteration: 3 #> # nbIterations: 3 #> # experimentsUsedSoFar: 39 #> # timeUsed: 0 #> # remainingBudget: 3 #> # currentBudget: 3 #> # number of elites: 2 #> # nbConfigurations: 2  # best performing hyperparameter configuration instance$result #>         cp configuration learner_param_vals  x_domain classif.ce #> 1: -3.4067             4          <list[2]> <list[1]>  0.2441406  # all evaluated hyperparameter configuration as.data.table(instance$archive) #>          cp classif.ce x_domain_cp runtime_learners           timestamp #>  1: -5.0604  0.2343750 0.006343022            0.015 2021-12-26 04:26:43 #>  2: -3.6055  0.2500000 0.027173854            0.017 2021-12-26 04:26:43 #>  3: -2.7227  0.3085938 0.065697132            0.015 2021-12-26 04:26:43 #>  4: -5.0604  0.2070312 0.006343022            0.014 2021-12-26 04:26:43 #>  5: -3.6055  0.2656250 0.027173854            0.014 2021-12-26 04:26:43 #>  6: -2.7227  0.2851562 0.065697132            0.015 2021-12-26 04:26:43 #>  7: -5.0604  0.2812500 0.006343022            0.014 2021-12-26 04:26:43 #>  8: -3.6055  0.2617188 0.027173854            0.033 2021-12-26 04:26:43 #>  9: -2.7227  0.2617188 0.065697132            0.014 2021-12-26 04:26:43 #> 10: -5.0604  0.2109375 0.006343022            0.013 2021-12-26 04:26:43 #> 11: -3.6055  0.2226562 0.027173854            0.016 2021-12-26 04:26:43 #> 12: -2.7227  0.2031250 0.065697132            0.014 2021-12-26 04:26:43 #> 13: -5.0604  0.2617188 0.006343022            0.014 2021-12-26 04:26:43 #> 14: -3.6055  0.2226562 0.027173854            0.014 2021-12-26 04:26:43 #> 15: -2.7227  0.2226562 0.065697132            0.013 2021-12-26 04:26:43 #> 16: -5.0604  0.2656250 0.006343022            0.018 2021-12-26 04:26:44 #> 17: -3.6055  0.2421875 0.027173854            0.016 2021-12-26 04:26:44 #> 18: -2.7227  0.2617188 0.065697132            0.014 2021-12-26 04:26:44 #> 19: -5.0604  0.2890625 0.006343022            0.035 2021-12-26 04:26:44 #> 20: -3.6055  0.2421875 0.027173854            0.016 2021-12-26 04:26:44 #> 21: -2.7227  0.2421875 0.065697132            0.014 2021-12-26 04:26:44 #> 22: -3.6055  0.2656250 0.027173854            0.017 2021-12-26 04:26:44 #> 23: -2.7227  0.2656250 0.065697132            0.019 2021-12-26 04:26:44 #> 24: -3.4067  0.2656250 0.033150416            0.017 2021-12-26 04:26:44 #> 25: -3.6850  0.2656250 0.025097175            0.016 2021-12-26 04:26:44 #> 26: -3.4067  0.2421875 0.033150416            0.016 2021-12-26 04:26:45 #> 27: -3.6850  0.2421875 0.025097175            0.015 2021-12-26 04:26:45 #> 28: -3.4067  0.2421875 0.033150416            0.014 2021-12-26 04:26:45 #> 29: -3.6850  0.2421875 0.025097175            0.015 2021-12-26 04:26:45 #> 30: -3.4067  0.2656250 0.033150416            0.014 2021-12-26 04:26:45 #> 31: -3.6850  0.2656250 0.025097175            0.018 2021-12-26 04:26:45 #> 32: -3.4067  0.2500000 0.033150416            0.016 2021-12-26 04:26:45 #> 33: -3.6850  0.2500000 0.025097175            0.015 2021-12-26 04:26:45 #> 34: -3.4067  0.2617188 0.033150416            0.020 2021-12-26 04:26:45 #> 35: -3.6850  0.2617188 0.025097175            0.017 2021-12-26 04:26:45 #> 36: -3.4067  0.2031250 0.033150416            0.019 2021-12-26 04:26:45 #> 37: -3.6850  0.2031250 0.025097175            0.015 2021-12-26 04:26:45 #> 38: -3.4067  0.2226562 0.033150416            0.018 2021-12-26 04:26:46 #> 39: -3.6850  0.2226562 0.025097175            0.015 2021-12-26 04:26:46 #>          cp classif.ce x_domain_cp runtime_learners           timestamp #>     batch_nr race step instance configuration      resample_result #>  1:        1    1    1       10             1 <ResampleResult[22]> #>  2:        1    1    1       10             2 <ResampleResult[22]> #>  3:        1    1    1       10             3 <ResampleResult[22]> #>  4:        2    1    1        9             1 <ResampleResult[22]> #>  5:        2    1    1        9             2 <ResampleResult[22]> #>  6:        2    1    1        9             3 <ResampleResult[22]> #>  7:        3    1    1        6             1 <ResampleResult[22]> #>  8:        3    1    1        6             2 <ResampleResult[22]> #>  9:        3    1    1        6             3 <ResampleResult[22]> #> 10:        4    1    1        8             1 <ResampleResult[22]> #> 11:        4    1    1        8             2 <ResampleResult[22]> #> 12:        4    1    1        8             3 <ResampleResult[22]> #> 13:        5    1    1        5             1 <ResampleResult[22]> #> 14:        5    1    1        5             2 <ResampleResult[22]> #> 15:        5    1    1        5             3 <ResampleResult[22]> #> 16:        6    1    1        7             1 <ResampleResult[22]> #> 17:        6    1    1        7             2 <ResampleResult[22]> #> 18:        6    1    1        7             3 <ResampleResult[22]> #> 19:        7    1    1        1             1 <ResampleResult[22]> #> 20:        7    1    1        1             2 <ResampleResult[22]> #> 21:        7    1    1        1             3 <ResampleResult[22]> #> 22:        8    2    1        3             2 <ResampleResult[22]> #> 23:        8    2    1        3             3 <ResampleResult[22]> #> 24:        8    2    1        3             4 <ResampleResult[22]> #> 25:        8    2    1        3             5 <ResampleResult[22]> #> 26:        9    2    1        1             4 <ResampleResult[22]> #> 27:        9    2    1        1             5 <ResampleResult[22]> #> 28:       10    2    1        7             4 <ResampleResult[22]> #> 29:       10    2    1        7             5 <ResampleResult[22]> #> 30:       11    2    1        9             4 <ResampleResult[22]> #> 31:       11    2    1        9             5 <ResampleResult[22]> #> 32:       12    2    1       10             4 <ResampleResult[22]> #> 33:       12    2    1       10             5 <ResampleResult[22]> #> 34:       13    2    1        6             4 <ResampleResult[22]> #> 35:       13    2    1        6             5 <ResampleResult[22]> #> 36:       14    2    1        8             4 <ResampleResult[22]> #> 37:       14    2    1        8             5 <ResampleResult[22]> #> 38:       15    2    1        5             4 <ResampleResult[22]> #> 39:       15    2    1        5             5 <ResampleResult[22]> #>     batch_nr race step instance configuration      resample_result  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(task)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"TunerNLoptr class implements non-linear optimization. Calls nloptr::nloptr package nloptr.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"Johnson, G S (2020). âNLopt nonlinear-optimization package.â https://github.com/stevengj/nlopt.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"termination conditions stopval, maxtime maxeval nloptr::nloptr() deactivated replaced bbotk::Terminator subclasses. x function value tolerance termination conditions (xtol_rel = 10^-4, xtol_abs = rep(0.0, length(x0)), ftol_rel = 0.0 ftol_abs = 0.0) still available implemented package defaults. deactivate conditions, set -1.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"Tuner can instantiated via dictionary mlr_tuners associated sugar function tnr():","code":"TunerNLoptr$new() mlr_tuners$get(\"nloptr\") tnr(\"nloptr\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"algorithm character(1) eval_g_ineq function() xtol_rel numeric(1) xtol_abs numeric(1) ftol_rel numeric(1) ftol_abs numeric(1) start_values character(1) Create random start values based center search space? latter case, center parameters trafo applied. meaning control parameters, see nloptr::nloptr() nloptr::nloptr.print.options(). termination conditions stopval, maxtime maxeval nloptr::nloptr() deactivated replaced Terminator subclasses. x function value tolerance termination conditions (xtol_rel = 10^-4, xtol_abs = rep(0.0, length(x0)), ftol_rel = 0.0 ftol_abs = 0.0) still available implemented package defaults. deactivate conditions, set -1.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"mlr3tuning::Tuner -> mlr3tuning::TunerFromOptimizer -> TunerNLoptr","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$print() mlr3tuning::TunerFromOptimizer$optimize()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"TunerNLoptr$new() TunerNLoptr$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"","code":"TunerNLoptr$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"","code":"TunerNLoptr$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_nloptr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter Tuning with Non-linear Optimization â mlr_tuners_nloptr","text":"","code":"if (FALSE) { # retrieve task task = tsk(\"pima\")  # load learner and set search space learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE))  # hyperparameter tuning on the pima indians diabetes data set instance = tune(   method = \"nloptr\",   task = task,   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   algorithm = \"NLOPT_LN_BOBYQA\" )  # best performing hyperparameter configuration instance$result  # all evaluated hyperparameter configuration as.data.table(instance$archive)  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(task) }"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":null,"dir":"Reference","previous_headings":"","what":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"Subclass random search tuning. random points sampled paradox::generate_design_random().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"Bergstra J, Bengio Y (2012). âRandom Search Hyper-Parameter Optimization.â Journal Machine Learning Research, 13(10), 281--305. https://jmlr.csail.mit.edu/papers/v13/bergstra12a.html.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"dictionary","dir":"Reference","previous_headings":"","what":"Dictionary","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"Tuner can instantiated via dictionary mlr_tuners associated sugar function tnr():","code":"TunerRandomSearch$new() mlr_tuners$get(\"random_search\") tnr(\"random_search\")"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"parallelization","dir":"Reference","previous_headings":"","what":"Parallelization","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"order support general termination criteria parallelization, evaluate points batch-fashion size batch_size. Larger batches mean can parallelize , smaller batches imply fine-grained checking termination criteria. batch contains batch_size times resampling$iters jobs. E.g., set batch size 10 points 5-fold cross validation, can utilize 50 cores. Parallelization supported via package future (see mlr3::benchmark()'s section parallelization details).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"logging","dir":"Reference","previous_headings":"","what":"Logging","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"Tuners use logger (implemented lgr) package bbotk. Use lgr::get_logger(\"bbotk\") access control logger.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"parameters","dir":"Reference","previous_headings":"","what":"Parameters","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"batch_size integer(1) Maximum number points try batch.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"progress-bars","dir":"Reference","previous_headings":"","what":"Progress Bars","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"$optimize() supports progress bars via package progressr combined Terminator. Simply wrap function progressr::with_progress() enable . recommend use package progress backend; enable progressr::handlers(\"progress\").","code":""},{"path":[]},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"mlr3tuning::Tuner -> mlr3tuning::TunerFromOptimizer -> TunerRandomSearch","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"mlr3tuning::Tuner$format() mlr3tuning::Tuner$print() mlr3tuning::TunerFromOptimizer$optimize()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"TunerRandomSearch$new() TunerRandomSearch$clone()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"Creates new instance R6 class.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"","code":"TunerRandomSearch$new()"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"objects class cloneable method.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"","code":"TunerRandomSearch$clone(deep = FALSE)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"deep Whether make deep clone.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/mlr_tuners_random_search.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hyperparameter Tuning with Random Search â mlr_tuners_random_search","text":"","code":"# retrieve task task = tsk(\"pima\")  # load learner and set search space learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE))  # hyperparameter tuning on the pima indians diabetes data set instance = tune(   method = \"random_search\",   task = task,   learner = learner,   resampling = rsmp(\"holdout\"),   measure = msr(\"classif.ce\"),   term_evals = 10 )  # best performing hyperparameter configuration instance$result #>          cp learner_param_vals  x_domain classif.ce #> 1: -8.83415          <list[2]> <list[1]>   0.265625  # all evaluated hyperparameter configuration as.data.table(instance$archive) #>            cp classif.ce  x_domain_cp runtime_learners           timestamp #>  1: -3.601202  0.2734375 0.0272908946            0.016 2021-12-26 04:26:47 #>  2: -8.834150  0.2656250 0.0001456725            0.017 2021-12-26 04:26:47 #>  3: -4.961830  0.2656250 0.0070001033            0.018 2021-12-26 04:26:47 #>  4: -5.549493  0.2656250 0.0038894273            0.015 2021-12-26 04:26:47 #>  5: -3.148538  0.2734375 0.0429148251            0.018 2021-12-26 04:26:47 #>  6: -3.941600  0.2656250 0.0194171198            0.016 2021-12-26 04:26:48 #>  7: -4.554283  0.2656250 0.0105220413            0.016 2021-12-26 04:26:48 #>  8: -4.002517  0.2656250 0.0182695937            0.015 2021-12-26 04:26:48 #>  9: -8.475429  0.2656250 0.0002085298            0.016 2021-12-26 04:26:48 #> 10: -2.553312  0.2734375 0.0778234791            0.015 2021-12-26 04:26:48 #>     batch_nr      resample_result #>  1:        1 <ResampleResult[22]> #>  2:        2 <ResampleResult[22]> #>  3:        3 <ResampleResult[22]> #>  4:        4 <ResampleResult[22]> #>  5:        5 <ResampleResult[22]> #>  6:        6 <ResampleResult[22]> #>  7:        7 <ResampleResult[22]> #>  8:        8 <ResampleResult[22]> #>  9:        9 <ResampleResult[22]> #> 10:       10 <ResampleResult[22]>  # fit final model on complete data set learner$param_set$values = instance$result_learner_param_vals learner$train(task)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages â reexports","title":"Objects exported from other packages â reexports","text":"objects imported packages. Follow links see documentation. bbotk mlr_terminators, trm, trms","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tnr.html","id":null,"dir":"Reference","previous_headings":"","what":"Syntactic Sugar for Tuner Construction â tnr","title":"Syntactic Sugar for Tuner Construction â tnr","text":"function complements mlr_tuners functions spirit mlr3::mlr_sugar.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tnr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Syntactic Sugar for Tuner Construction â tnr","text":"","code":"tnr(.key, ...)  tnrs(.keys, ...)"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tnr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Syntactic Sugar for Tuner Construction â tnr","text":".key (character(1)) Key passed respective dictionary retrieve object. ... (named list()) Named arguments passed constructor, set parameters paradox::ParamSet, set public field. See mlr3misc::dictionary_sugar_get() details. .keys (character()) Keys passed respective dictionary retrieve multiple objects.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tnr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Syntactic Sugar for Tuner Construction â tnr","text":"Tuner tnr() list Tuner tnrs()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tnr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Syntactic Sugar for Tuner Construction â tnr","text":"","code":"tnr(\"random_search\") #> <TunerRandomSearch> #> * Parameters: batch_size=1 #> * Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct #> * Properties: dependencies, single-crit, multi-crit #> * Packages: mlr3tuning, bbotk"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for Tuning â tune","title":"Function for Tuning â tune","text":"Function tune mlr3::Learner.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for Tuning â tune","text":"","code":"tune(   method,   task,   learner,   resampling,   measures,   term_evals = NULL,   term_time = NULL,   search_space = NULL,   store_models = FALSE,   allow_hotstart = FALSE,   ... )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for Tuning â tune","text":"method (character(1)) Key retrieve tuner mlr_tuners dictionary. task (mlr3::Task) Task operate . learner (mlr3::Learner) Learner tune. resampling (mlr3::Resampling) Resampling used evaluated performance hyperparameter configurations. Uninstantiated resamplings instantiated construction configurations evaluated data splits. Already instantiated resamplings kept unchanged. Specialized Tuner change resampling e.g. evaluate hyperparameter configuration different data splits. field, however, always returns resampling passed construction. measures (list mlr3::Measure) Measures optimize. term_evals (integer(1)) Number allowed evaluations. term_time (integer(1)) Maximum allowed time seconds. search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). store_models (logical(1)) TRUE, fitted models stored benchmark result (archive$benchmark_result). store_benchmark_result = FALSE, models stored temporarily accessible tuning. combination needed measures require model. allow_hotstart (logical(1)) Allow hotstart learners previously fitted models. See also mlr3::HotstartStack. learner must support hotstarting. Sets store_models = TRUE. ... (named list()) Named arguments set parameters tuner.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for Tuning â tune","text":"TuningInstanceSingleCrit | TuningInstanceMultiCrit","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function for Tuning â tune","text":"","code":"learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE))  instance = tune(   method = \"random_search\",   task = tsk(\"pima\"),   learner = learner,   resampling = rsmp (\"holdout\"),   measures = msr(\"classif.ce\"),   term_evals = 4)  # apply hyperparameter values to learner learner$param_set$values = instance$result_learner_param_vals"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune_nested.html","id":null,"dir":"Reference","previous_headings":"","what":"Function for Nested Resampling â tune_nested","title":"Function for Nested Resampling â tune_nested","text":"Function conduct nested resampling.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune_nested.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function for Nested Resampling â tune_nested","text":"","code":"tune_nested(   method,   task,   learner,   inner_resampling,   outer_resampling,   measure,   term_evals = NULL,   term_time = NULL,   search_space = NULL,   ... )"},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune_nested.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function for Nested Resampling â tune_nested","text":"method (character(1)) Key retrieve tuner mlr_tuners dictionary. task (mlr3::Task) Task operate . learner (mlr3::Learner) Learner tune. inner_resampling (mlr3::Resampling) Resampling used inner loop. outer_resampling mlr3::Resampling) Resampling used outer loop. measure (mlr3::Measure) Measure optimize. term_evals (integer(1)) Number allowed evaluations. term_time (integer(1)) Maximum allowed time seconds. search_space (paradox::ParamSet) Hyperparameter search space. NULL (default), search space constructed TuneToken learner's parameter set (learner$param_set). ... (named list()) Named arguments set parameters tuner.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune_nested.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function for Nested Resampling â tune_nested","text":"mlr3::ResampleResult","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/reference/tune_nested.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function for Nested Resampling â tune_nested","text":"","code":"rr = tune_nested(   method = \"random_search\",   task = tsk(\"pima\"),   learner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1, logscale = TRUE)),    inner_resampling = rsmp (\"holdout\"),   outer_resampling = rsmp(\"cv\", folds = 2),    measure = msr(\"classif.ce\"),   term_evals = 2,   batch_size = 2)  # retrieve inner tuning results. extract_inner_tuning_results(rr) #>    iteration        cp classif.ce learner_param_vals  x_domain task_id #> 1:         1 -3.637954  0.2421875          <list[2]> <list[1]>    pima #> 2:         2 -7.646248  0.2187500          <list[2]> <list[1]>    pima #>             learner_id resampling_id #> 1: classif.rpart.tuned            cv #> 2: classif.rpart.tuned            cv  # performance scores estimated on the outer resampling rr$score() #>                 task task_id         learner          learner_id #> 1: <TaskClassif[49]>    pima <AutoTuner[41]> classif.rpart.tuned #> 2: <TaskClassif[49]>    pima <AutoTuner[41]> classif.rpart.tuned #>            resampling resampling_id iteration              prediction #> 1: <ResamplingCV[19]>            cv         1 <PredictionClassif[20]> #> 2: <ResamplingCV[19]>            cv         2 <PredictionClassif[20]> #>    classif.ce #> 1:  0.2500000 #> 2:  0.2760417  # unbiased performance of the final model trained on the full data set rr$aggregate() #> classif.ce  #>  0.2630208"},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-0909000","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.9.0.9000","title":"mlr3tuning 0.9.0.9000","text":"Fixes bug TuningInstanceMultiCrit$assign_result(). Hotstarting learners previously fitted models.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-090","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.9.0","title":"mlr3tuning 0.9.0","text":"CRAN release: 2021-09-14 Adds AutoTuner$base_learner() method extract base learner nested learner objects. tune() supports multi-criteria tuning. Allows empty search space. Adds TunerIrace irace package. extract_inner_tuning_archives() helper function extract inner tuning archives. Removes ArchiveTuning$extended_archive() method. mlr3::ResampleResults joined automatically .data.table.TuningArchive() extract_inner_tuning_archives().","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-080","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.8.0","title":"mlr3tuning 0.8.0","text":"CRAN release: 2021-03-12 Adds tune(), auto_tuner() tune_nested() sugar functions. TuningInstanceSingleCrit, TuningInstanceMultiCrit AutoTuner can initialized store_benchmark_result = FALSE store_models = TRUE allow measures access models. Prettier printing methods.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-070","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.7.0","title":"mlr3tuning 0.7.0","text":"CRAN release: 2021-02-11 Fix TuningInstance*$assign_result() errors required parameter bug. Shortcuts access $learner(), $learners(), $learner_param_vals(), $predictions() $resample_result() benchmark result archive. extract_inner_tuning_results() helper function extract inner tuning results.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-060","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.6.0","title":"mlr3tuning 0.6.0","text":"CRAN release: 2021-01-24 ArchiveTuning$data public field now.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-050","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.5.0","title":"mlr3tuning 0.5.0","text":"CRAN release: 2020-12-07 Adds TunerCmaes adagio package. Fix predict_type AutoTuner. Support set TuneToken Learner$param_set create search space . order parameters TuningInstanceSingleCrit TuningInstanceSingleCrit changed.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-040","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.4.0","title":"mlr3tuning 0.4.0","text":"CRAN release: 2020-10-07 Option control store_benchmark_result, store_models check_values AutoTuner. store_tuning_instance must set parameter initialization. Fixes check_values flag TuningInstanceSingleCrit TuningInstanceMultiCrit. Removed dependency orphaned package bibtex.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-030","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.3.0","title":"mlr3tuning 0.3.0","text":"CRAN release: 2020-09-08 Compact -memory representation R6 objects save space saving mlr3 objects via saveRDS(), serialize() etc. Archive ArchiveTuning now stores benchmark result $benchmark_result. change removed resample results archive can still accessed via benchmark result. Warning message external package tuning installed. retrieve inner tuning results nested resampling, .data.table(rr)$learner[[1]]$tuning_result must used now.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-020","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.2.0","title":"mlr3tuning 0.2.0","text":"CRAN release: 2020-07-28 TuningInstance now TuningInstanceSingleCrit. TuningInstanceMultiCrit still available multi-criteria tuning. Terminators now accessible trm() trms() instead term() terms(). Storing resample results optional now using store_resample_result flag TuningInstanceSingleCrit TuningInstanceMultiCrit TunerNLoptr adds non-linear optimization nloptr package. Logging controlled bbotk logger now. Proposed points performance values can checked validity activating check_values flag TuningInstanceSingleCrit TuningInstanceMultiCrit.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-013","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.1.3","title":"mlr3tuning 0.1.3","text":"mlr3tuning now depends bbotk package basic tuning objects. Terminator classes now live bbotk. consequence ObjectiveTuning inherits bbotk::Objective, TuningInstance bbotk::OptimInstance Tuner bbotk::Optimizer TuningInstance$param_set becomes TuningInstance$search_space avoid confusion param_set usually contains parameters change behaviour object. Tuning triggered $optimize() instead $tune()","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-012","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.1.2","title":"mlr3tuning 0.1.2","text":"CRAN release: 2020-01-31 Fixed bug AutoTuner $clone() missing. Tuning results unaffected, stored models contained wrong hyperparameter values (#223). Improved output log (#218).","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-011","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.1.1","title":"mlr3tuning 0.1.1","text":"CRAN release: 2019-12-06 Maintenance release.","code":""},{"path":"https://mlr3tuning.mlr-org.com/dev/news/index.html","id":"mlr3tuning-010","dir":"Changelog","previous_headings":"","what":"mlr3tuning 0.1.0","title":"mlr3tuning 0.1.0","text":"CRAN release: 2019-09-30 Initial prototype.","code":""}]
