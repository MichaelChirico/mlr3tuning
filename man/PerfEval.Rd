% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/PerfEval.R
\docType{data}
\name{PerfEval}
\alias{PerfEval}
\title{PerfEval Class}
\format{\link[R6:R6Class]{R6::R6Class} object.}
\description{
Implements a performance evaluator for tuning. This class encodes the black box objective function,
that a generic tuner has to optimize. It allows the basic operations of querying the objective
at design points (see \code{eval_batch}), storing the evaluated point in an internal archive
and querying the archive (see \code{archive}).

The performance evaluator is one of the major inputs for constructing a \link{Tuner}.
}
\section{Construction}{
\preformatted{pe = PerfEval$new(task, learner, resampling, measures, param_set,
  store_models = FALSE)
}
\itemize{
\item \code{task} :: \link[mlr3:Task]{mlr3::Task}.
\item \code{learner} :: \link[mlr3:Learner]{mlr3::Learner}.
\item \code{resampling} :: \link[mlr3:Resampling]{mlr3::Resampling}.
\item \code{measures} :: list of \link[mlr3:Measure]{mlr3::Measure}.
\item \code{param_set} :: \link[paradox:ParamSet]{paradox::ParamSet}.
\item \code{store_models} :: \code{logical(1)}\cr
Keep the fitted learner models? Passed down to \code{\link[mlr3:benchmark]{mlr3::benchmark()}}.
}
}

\section{Fields}{

\itemize{
\item \code{task} :: \link[mlr3:Task]{mlr3::Task}\cr
Stored task.
\item \code{learner} :: \link[mlr3:Learner]{mlr3::Learner}\cr
Stored learner.
\item \code{resampling} :: \link[mlr3:Resampling]{mlr3::Resampling}\cr
Stored resampling
\item \code{measures} :: list of \link[mlr3:Measure]{mlr3::Measure}\cr
Stored measures.
\item \code{param_set} :: \link[paradox:ParamSet]{paradox::ParamSet}\cr
Stored parameter set.
\item \code{bmr} :: \link[mlr3:BenchmarkResult]{mlr3::BenchmarkResult}\cr
A benchmark result, which is used as data storage.
\item \code{n_evals} :: \code{integer(1)}\cr
Number of unique experiments stored in the container.
}
}

\section{Methods}{

\itemize{
\item \code{eval_batch(dt)}\cr
\code{\link[data.table:data.table]{data.table::data.table()}} -> \link[data.table:data.table]{data.table::data.table}\cr
Evaluates all hyperparameter configurations in \code{dt} through resampling, where each configuration is a row, and columns are scalar parameters.
Return a data.table with corresponding rows, where each column is an named measure.
\item \code{best()}\cr
() -> \link[mlr3:ResampleResult]{mlr3::ResampleResult}\cr
Queries the \link[mlr3:BenchmarkResult]{mlr3::BenchmarkResult} for the best \link[mlr3:ResampleResult]{mlr3::ResampleResult} according to the
first measure in \code{$measures}.
\item \code{archive(unnest = TRUE)}
\code{logical(1)} -> \code{\link[data.table:data.table]{data.table::data.table()}}\cr
Returns a table of contained resample results, similar to the one returned by \code{\link[mlr3:benchmark]{mlr3::benchmark()}}'s
\code{archive()} method. If \code{unnest} is \code{TRUE}, hyperparameter settings are stored in
separate columns instead of inside a list column
}
}

\examples{
library(mlr3)
library(paradox)
library(data.table)
# Object required to define the performance evaluator:
task = mlr_tasks$get("iris")
learner = mlr_learners$get("classif.rpart")
resampling = mlr_resamplings$get("holdout")
measures = mlr_measures$mget("classif.ce")
param_set = ParamSet$new(params = list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)))

pe = PerfEval$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measures = measures,
  param_set = param_set
)

pe$eval_batch(data.table(cp = c(0.05, 0.01), minsplit = c(5, 3)))
pe$best()
}
\concept{PerfEval}
\keyword{datasets}
