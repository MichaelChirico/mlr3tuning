% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/PerfEval.R
\docType{data}
\name{PerfEval}
\alias{PerfEval}
\title{PerfEval Class}
\format{\link[R6:R6Class]{R6::R6Class} object.}
\description{
Implements a performance evaluator for tuning. This class encodes the black box objective function,
that a generic tuner has to optimize. It allows the basic operations of querying the objective
at design points, storing the evaluated point in an internal archive and querying the archive.

The performance evaluator is one of the major inputs for constructing a \link{Tuner}.
}
\section{Construction}{
\preformatted{pe = PerfEval$new(task, learner, resampling, measures, param_set,
  ctrl = list())
}
\itemize{
\item \code{task} :: \link[mlr3:Task]{mlr3::Task}.
\item \code{learner} :: \link[mlr3:Learner]{mlr3::Learner}.
\item \code{resampling} :: \link[mlr3:Resampling]{mlr3::Resampling}.
\item \code{measures} :: list of \link[mlr3:Measure]{mlr3::Measure}.
\item \code{param_set} :: \link[paradox:ParamSet]{paradox::ParamSet}.
\item \code{ctrl} :: named \code{list()}\cr
See \code{\link[mlr3:mlr_control]{mlr3::mlr_control()}}.
}
}

\section{Fields}{

\itemize{
\item \code{task} :: \link[mlr3:Task]{mlr3::Task}\cr
Stored task.
\item \code{learner} :: \link[mlr3:Learner]{mlr3::Learner}\cr
Stored learner.
\item \code{resampling} :: \link[mlr3:Resampling]{mlr3::Resampling}\cr
Stored resampling
\item \code{measures} :: list of \link[mlr3:Measure]{mlr3::Measure}\cr
Stored measures.
\item \code{param_set} :: \link[paradox:ParamSet]{paradox::ParamSet}\cr
Stored parameter set.
\item \code{bmr} :: \link[mlr3:BenchmarkResult]{mlr3::BenchmarkResult}\cr
A benchmark result, which is used as data storage.
\item \code{hooks} :: \code{list()}\cr
List of functions that are executed with \code{run_hooks()} for evaluation.
This is for internal use.
}
}

\section{Methods}{

\itemize{
\item \code{eval(dt)}\cr
\code{\link[data.table:data.table]{data.table::data.table()}} -> \code{self}\cr
Evaluates all hyperparameter configurations in \code{dt}.
Each configuration is a row.
\item \code{eval(design)}\cr
\link[paradox:Design]{paradox::Design} -> \code{self}\cr
Evaluates all configurations defined by the design.
\item \code{best()}\cr
() -> \link[mlr3:ResampleResult]{mlr3::ResampleResult}\cr
Queries the \link[mlr3:BenchmarkResult]{mlr3::BenchmarkResult} for the best \link[mlr3:ResampleResult]{mlr3::ResampleResult} according to the
first measure in \code{$measures}.
\item \code{run_hooks()}\cr
\code{()} -> \code{NULL}\cr
Runs all hook functions. For internal use.
\item \code{add_hook(hook)}\cr
\code{function()} -> \code{NULL}\cr
Adds a hook function. For internal use.
}
}

\examples{
library(mlr3)
library(paradox)
# Object required to define the performance evaluator:
task = mlr_tasks$get("iris")
learner = mlr_learners$get("classif.rpart")
resampling = mlr_resamplings$get("holdout")
measures = mlr_measures$mget("classif.ce")
param_set = ParamSet$new(params = list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)))

pe = PerfEval$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measures = measures,
  param_set = param_set
)

pe$eval(data.table::data.table(cp = 0.05, minsplit = 5))
pe$eval(data.table::data.table(cp = 0.01, minsplit = 3))
pe$best()
}
\concept{PerfEval}
\keyword{datasets}
