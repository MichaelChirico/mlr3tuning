% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AutoTuner.R
\docType{data}
\name{AutoTuner}
\alias{AutoTuner}
\title{AutoTuner}
\format{\link[R6:R6Class]{R6::R6Class} object inheriting from \link[mlr3:Learner]{mlr3::Learner}.}
\description{
The \code{AutoTuner} is a \link[mlr3:Learner]{mlr3::Learner} which tunes a subordinate learner via resampling.
The best found configuration is then used to train a model on the complete training data.
Note that this class allows to perform nested resampling by passing an \link{AutoTuner} object to \code{\link[mlr3:resample]{mlr3::resample()}}
or \code{\link[mlr3:benchmark]{mlr3::benchmark()}}.
}
\section{Construction}{
\preformatted{at = AutoTuner$new(learner, resampling, measures, param_set, terminator,
  tuner, tuner_settings, ctrl = list(), id = "autotuner")
}
\itemize{
\item \code{learner} :: \link[mlr3:Learner]{mlr3::Learner}\cr
Subordinate learner to tune.
\item \code{resampling} :: \link[mlr3:Resampling]{mlr3::Resampling}\cr
Resampling strategy used to assess the performance of the learner on the (subset of) the
\link{Task} passed to \code{$train()}.
\item \code{measures} :: list of \link[mlr3:Measure]{mlr3::Measure}\cr
Performance measures. The first one is subject to tuning.
\item \code{param_set} :: \link[paradox:ParamSet]{paradox::ParamSet}\cr
Tuning space.
\item \code{terminator} :: \link{Terminator}\cr
Controls the terminator of the \code{tuner}.
\item \code{tuner} :: \link{Tuner}\cr
Uninitialized tuner factory, e.g. TunerGridSearch.
\item \code{tuner_settings} :: named \code{list()}\cr
List with tuner settings (e.g. see \link{TunerGridSearch})
\item \code{ctrl} :: named \code{list()}\cr
See \code{\link[mlr3:mlr_control]{mlr3::mlr_control()}}.
\item \code{id} :: \code{character(1)}\cr
Name of the learner.
}
}

\section{Fields}{

All fields from \link{Learner}, and additionally:
\itemize{
\item \code{learner} :: \link[mlr3:Learner]{mlr3::Learner}\cr
Subordinate learner. After \code{train()} of the \code{AutoTuner} has been executed,
this learner stores the final model on is parametrized with the best found solution.
\item \code{store_bmr} :: \code{logical(1)}\cr
If \code{TRUE}, store the benchmark result as slot \code{$bmr}.
\item \code{bmr} :: \link[mlr3:BenchmarkResult]{mlr3::BenchmarkResult}\cr
Only stored if \code{store_bmr} has been set to \code{TRUE}.
This object acts as an optimization path.
\item \code{tuner} :: \link{Tuner}\cr
Access to the stored \link{Tuner}.
\item \code{tune_path} :: \code{\link[data.table:data.table]{data.table::data.table()}}\cr
Only stored if \code{store_bmr} has been set to \code{TRUE}.
This is the archive of the stored \link[mlr3:BenchmarkResult]{mlr3::BenchmarkResult} with hyperparameters as separate columns.
}
}

\section{Methods}{

See \link[mlr3:Learner]{mlr3::Learner}.
}

\examples{
library(mlr3)
library(paradox)
task = mlr_tasks$get("iris")
learner = mlr_learners$get("classif.rpart")
resampling = mlr_resamplings$get("holdout")
measures = mlr_measures$mget("classif.ce")
param_set = ParamSet$new(
  params = list(ParamDbl$new("cp", lower = 0.001, upper = 0.1)))

terminator = TerminatorEvaluations$new(5)

at = AutoTuner$new(learner, resampling, measures, param_set, terminator, tuner = TunerGridSearch,
  tuner_settings = list(resolution = 10L))
at$store_bmr = TRUE

at$train(task)
at$model
at$learner

# retrieve the best ResampleResult
rr = at$bmr$best(measures)
rr$archive(measures)
}
\concept{Learner}
\keyword{datasets}
