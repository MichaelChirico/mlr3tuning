% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/PerformanceEvaluator.R
\name{PerformanceEvaluator}
\alias{PerformanceEvaluator}
\title{PerformanceEvaluator Class}
\description{
Implements a performance evaluator for \pkg{mlr3} as \code{R6} class \code{PerformanceEvaluator}. An object of that class
contains all relevant informations that are necessary to conduct tuning (\code{mlr3::Task}, \code{mlr3::Learner}, \code{mlr3::Resampling}, \code{mlr3::Measure}s,
\code{paradox::ParamSet}).
After defining a performance evaluator, we can use it to predict the generalization error of a specific learner configuration
defined by it's hyperparameter (using \code{$eval()}).
The \code{PerformanceEvaluator} class is the basis for further tuning strategies, i.e., grid or random search.
}
\section{Usage}{
\preformatted{# Construction
pe = PerformanceEvaluator$new(task, learner, resampling, param_set,
  ctrl = tune_control())

# Public members
pe$task
pe$learner
pe$resampling
pe$param_set
pe$ctrl
pe$hooks
pe$bmr

# Public methods
pe$eval(x)
pe$eval_vectorized(xts)
pe$get_best()
pe$run_hooks(id)
}
}

\section{Arguments}{

\itemize{
\item \code{task} (\code{mlr3::Task}):
The task that we want to evaluate.
\item \code{learner} (\code{mlr3::Learner}):
The learner that we want to evaluate.
\item \code{resampling} (\code{mlr3::Resampling}):
The Resampling method that is used to evaluate the learner.
\item \code{param_set} (\link[paradox:ParamSet]{paradox::ParamSet}):
Parameter set to define the hyperparameter space.
\item \code{ctrl} (\code{list()}):
See \code{\link[=tune_control]{tune_control()}}.
\item \code{xt} (\code{list()}):
A specific (transformed) parameter configuration given as named list (e.g. for rpart \code{list(cp = 0.05, minsplit = 4)}).
\item \code{xts} (\code{list()}):
Collection of multiple (transformed) parameter values gained that is, for example, gained from a tuning strategy like grid search (see \code{?paradox::generate_design_grid}).
\item \code{id} (\code{character(1)}):
Identifier of a hook.
}
}

\section{Details}{

\itemize{
\item \code{$new()} creates a new object of class \link{PerformanceEvaluator}.
\item \code{$task} (\code{mlr3::Task}) the task for which the tuning should be conducted.
\item \code{$learner} (\code{mlr3::Learner}) the algorithm for which the tuning should be conducted.
\item \code{$resampling} (\code{mlr3::Resampling}) strategy to evaluate a parameter setting
\item \code{$param_set} (\code{paradox::ParamSet}) parameter space given to the \code{Tuner} object to generate parameter values.
\item \code{$ctrl} (\code{list()}) execution control object for tuning (see \code{?tune_control}).
\item \code{$hooks} (\code{list()}) list of functions that could be executed with \code{run_hooks()}.
\item \code{$bmr} (\code{mlr3::BenchmarkResult}) object that contains all tuning results as \code{BenchmarkResult} object (see \code{?BenchmarkResult}).
\item \code{$eval(xt)} evaluates the (transformed) parameter setting \code{xt} (\code{list}) for the given learner and resampling.
\item \code{$eval_vectorized(xts)} performs resampling for multiple (transformed) parameter settings \code{xts} (list of lists).
\item \code{$get_best()}  get best parameter configuration from the \code{BenchmarkResult} object.
\item \code{$run_hooks()} run a function that runs on the whole \code{PerformanceEvaluator} object.
}
}

\examples{
# Object required to define the performance evaluator:
task = mlr3::mlr_tasks$get("iris")
learner = mlr3::mlr_learners$get("classif.rpart")
resampling = mlr3::mlr_resamplings$get("holdout")
measures = mlr3::mlr_measures$mget("classif.ce")
task$measures = measures
param_set = paradox::ParamSet$new(params = list(
  paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  paradox::ParamInt$new("minsplit", lower = 1, upper = 10)))

pe = PerformanceEvaluator$new(
  task = task,
  learner = learner,
  resampling = resampling,
  param_set = param_set
)

pe$eval(data.table::data.table(cp = 0.05, minsplit = 5))
pe$eval(data.table::data.table(cp = 0.01, minsplit = 3))
pe$get_best()
}
\concept{PerformanceEvaluator}
\keyword{internal}
